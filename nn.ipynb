{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7676d5d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:04.590238Z",
     "iopub.status.busy": "2024-07-12T13:26:04.589952Z",
     "iopub.status.idle": "2024-07-12T13:26:07.939882Z",
     "shell.execute_reply": "2024-07-12T13:26:07.938919Z"
    },
    "papermill": {
     "duration": 3.35764,
     "end_time": "2024-07-12T13:26:07.942390",
     "exception": false,
     "start_time": "2024-07-12T13:26:04.584750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816c2b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:07.956464Z",
     "iopub.status.busy": "2024-07-12T13:26:07.956056Z",
     "iopub.status.idle": "2024-07-12T13:26:28.277880Z",
     "shell.execute_reply": "2024-07-12T13:26:28.277038Z"
    },
    "papermill": {
     "duration": 20.331361,
     "end_time": "2024-07-12T13:26:28.280276",
     "exception": false,
     "start_time": "2024-07-12T13:26:07.948915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c292f3c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:28.297536Z",
     "iopub.status.busy": "2024-07-12T13:26:28.297115Z",
     "iopub.status.idle": "2024-07-12T13:26:28.301247Z",
     "shell.execute_reply": "2024-07-12T13:26:28.300377Z"
    },
    "papermill": {
     "duration": 0.014929,
     "end_time": "2024-07-12T13:26:28.303323",
     "exception": false,
     "start_time": "2024-07-12T13:26:28.288394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_pred = YOLO(\"D:\\BITS\\PS1\\pose\\posenet-python-master\\yolov8x-pose.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5e6dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:28.571542Z",
     "iopub.status.busy": "2024-07-12T13:26:28.571225Z",
     "iopub.status.idle": "2024-07-12T13:26:28.582151Z",
     "shell.execute_reply": "2024-07-12T13:26:28.581349Z"
    },
    "papermill": {
     "duration": 0.021422,
     "end_time": "2024-07-12T13:26:28.584010",
     "exception": false,
     "start_time": "2024-07-12T13:26:28.562588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_to_square(image):\n",
    "    height, width = image.shape[:2]\n",
    "    min_dimension = min(height, width)\n",
    "    x = int((width - min_dimension) / 2)\n",
    "    y = int((height - min_dimension) / 2)\n",
    "    cropped_image = image[y:y + min_dimension, x:x + min_dimension]\n",
    "    resized_image = cv2.resize(cropped_image, (640, 640))\n",
    "    return resized_image\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read image from {image_path}\")\n",
    "        return None\n",
    "    resized_image = (image)\n",
    "    predictions = model_pred.predict(resized_image)\n",
    "    return predictions\n",
    "\n",
    "def extract_keypoints(predictions):\n",
    "    keypoints = []\n",
    "    for result in predictions:\n",
    "        if result.keypoints is not None:\n",
    "            keypoints.append(result.keypoints.xyn.cpu().numpy().tolist())\n",
    "    return keypoints\n",
    "\n",
    "def process_dataset(dataset_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                file_path = os.path.join(root, file)\n",
    "                predictions = process_image(file_path)\n",
    "                if predictions:\n",
    "                    keypoints = extract_keypoints(predictions)\n",
    "                    label = os.path.basename(root)\n",
    "                    data.append(keypoints)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4574f665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:28.599750Z",
     "iopub.status.busy": "2024-07-12T13:26:28.599465Z",
     "iopub.status.idle": "2024-07-12T13:27:25.132261Z",
     "shell.execute_reply": "2024-07-12T13:27:25.131437Z"
    },
    "papermill": {
     "duration": 56.542846,
     "end_time": "2024-07-12T13:27:25.134113",
     "exception": false,
     "start_time": "2024-07-12T13:26:28.591267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 1826.4ms\n",
      "Speed: 3577.9ms preprocess, 1826.4ms inference, 836.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 220.5ms\n",
      "Speed: 2.0ms preprocess, 220.5ms inference, 7.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 255.2ms\n",
      "Speed: 3.9ms preprocess, 255.2ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 223.3ms\n",
      "Speed: 9.8ms preprocess, 223.3ms inference, 12.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 265.8ms\n",
      "Speed: 2.0ms preprocess, 265.8ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 384x640 1 person, 213.0ms\n",
      "Speed: 17.1ms preprocess, 213.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 224.1ms\n",
      "Speed: 5.5ms preprocess, 224.1ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 296.0ms\n",
      "Speed: 8.6ms preprocess, 296.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 227.0ms\n",
      "Speed: 2.3ms preprocess, 227.0ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 227.9ms\n",
      "Speed: 4.0ms preprocess, 227.9ms inference, 4.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 236.0ms\n",
      "Speed: 8.9ms preprocess, 236.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 223.2ms\n",
      "Speed: 3.0ms preprocess, 223.2ms inference, 5.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 302.9ms\n",
      "Speed: 4.8ms preprocess, 302.9ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 216.7ms\n",
      "Speed: 3.1ms preprocess, 216.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 230.6ms\n",
      "Speed: 4.0ms preprocess, 230.6ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 227.2ms\n",
      "Speed: 2.7ms preprocess, 227.2ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 223.9ms\n",
      "Speed: 2.6ms preprocess, 223.9ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 223.0ms\n",
      "Speed: 3.6ms preprocess, 223.0ms inference, 10.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 239.4ms\n",
      "Speed: 5.8ms preprocess, 239.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 246.0ms\n",
      "Speed: 3.2ms preprocess, 246.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 239.8ms\n",
      "Speed: 2.0ms preprocess, 239.8ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 231.1ms\n",
      "Speed: 2.4ms preprocess, 231.1ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 264.0ms\n",
      "Speed: 2.0ms preprocess, 264.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 230.4ms\n",
      "Speed: 2.1ms preprocess, 230.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 314.7ms\n",
      "Speed: 4.1ms preprocess, 314.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 239.6ms\n",
      "Speed: 5.4ms preprocess, 239.6ms inference, 4.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 247.1ms\n",
      "Speed: 6.8ms preprocess, 247.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 238.3ms\n",
      "Speed: 3.0ms preprocess, 238.3ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 245.2ms\n",
      "Speed: 4.0ms preprocess, 245.2ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 255.2ms\n",
      "Speed: 3.0ms preprocess, 255.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 235.7ms\n",
      "Speed: 10.5ms preprocess, 235.7ms inference, 3.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 231.9ms\n",
      "Speed: 2.1ms preprocess, 231.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 329.7ms\n",
      "Speed: 4.8ms preprocess, 329.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 250.3ms\n",
      "Speed: 6.5ms preprocess, 250.3ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 (no detections), 243.6ms\n",
      "Speed: 6.5ms preprocess, 243.6ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 214.7ms\n",
      "Speed: 2.8ms preprocess, 214.7ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 480x640 1 person, 267.6ms\n",
      "Speed: 4.4ms preprocess, 267.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 228.7ms\n",
      "Speed: 2.3ms preprocess, 228.7ms inference, 3.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 person, 267.0ms\n",
      "Speed: 2.3ms preprocess, 267.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 249.6ms\n",
      "Speed: 9.6ms preprocess, 249.6ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 304.6ms\n",
      "Speed: 10.9ms preprocess, 304.6ms inference, 3.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 person, 258.7ms\n",
      "Speed: 7.4ms preprocess, 258.7ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 235.4ms\n",
      "Speed: 3.3ms preprocess, 235.4ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 283.3ms\n",
      "Speed: 2.2ms preprocess, 283.3ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 283.2ms\n",
      "Speed: 3.9ms preprocess, 283.2ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 259.4ms\n",
      "Speed: 3.1ms preprocess, 259.4ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 256x640 1 person, 167.6ms\n",
      "Speed: 4.0ms preprocess, 167.6ms inference, 5.0ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 448x640 1 person, 261.1ms\n",
      "Speed: 2.1ms preprocess, 261.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 329.8ms\n",
      "Speed: 4.9ms preprocess, 329.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 229.6ms\n",
      "Speed: 2.1ms preprocess, 229.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 (no detections), 237.8ms\n",
      "Speed: 3.5ms preprocess, 237.8ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 329.0ms\n",
      "Speed: 2.2ms preprocess, 329.0ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 270.1ms\n",
      "Speed: 2.0ms preprocess, 270.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 259.0ms\n",
      "Speed: 3.5ms preprocess, 259.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 302.1ms\n",
      "Speed: 8.0ms preprocess, 302.1ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 241.0ms\n",
      "Speed: 2.5ms preprocess, 241.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 256x640 1 person, 172.5ms\n",
      "Speed: 4.1ms preprocess, 172.5ms inference, 4.8ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 640x640 1 person, 326.1ms\n",
      "Speed: 8.1ms preprocess, 326.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 246.1ms\n",
      "Speed: 2.3ms preprocess, 246.1ms inference, 3.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 231.3ms\n",
      "Speed: 2.0ms preprocess, 231.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 1 person, 315.1ms\n",
      "Speed: 13.2ms preprocess, 315.1ms inference, 2.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 416x640 1 person, 247.5ms\n",
      "Speed: 3.7ms preprocess, 247.5ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x512 1 person, 265.5ms\n",
      "Speed: 2.0ms preprocess, 265.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 384x640 1 person, 242.5ms\n",
      "Speed: 9.5ms preprocess, 242.5ms inference, 8.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 320x640 1 person, 241.4ms\n",
      "Speed: 3.6ms preprocess, 241.4ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 416x640 1 person, 286.9ms\n",
      "Speed: 2.8ms preprocess, 286.9ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 270.2ms\n",
      "Speed: 4.0ms preprocess, 270.2ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 235.1ms\n",
      "Speed: 4.0ms preprocess, 235.1ms inference, 9.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 371.4ms\n",
      "Speed: 4.6ms preprocess, 371.4ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 242.5ms\n",
      "Speed: 3.0ms preprocess, 242.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 2 persons, 550.9ms\n",
      "Speed: 6.4ms preprocess, 550.9ms inference, 2.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 (no detections), 288.3ms\n",
      "Speed: 5.5ms preprocess, 288.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 245.6ms\n",
      "Speed: 7.5ms preprocess, 245.6ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 758.7ms\n",
      "Speed: 5.6ms preprocess, 758.7ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 325.2ms\n",
      "Speed: 11.5ms preprocess, 325.2ms inference, 5.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 416x640 1 person, 547.5ms\n",
      "Speed: 3.6ms preprocess, 547.5ms inference, 6.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 261.6ms\n",
      "Speed: 7.0ms preprocess, 261.6ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 2 persons, 415.8ms\n",
      "Speed: 6.8ms preprocess, 415.8ms inference, 13.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 334.7ms\n",
      "Speed: 12.2ms preprocess, 334.7ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 263.9ms\n",
      "Speed: 3.6ms preprocess, 263.9ms inference, 7.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 576x640 2 persons, 453.7ms\n",
      "Speed: 8.5ms preprocess, 453.7ms inference, 11.9ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 416x640 1 person, 465.8ms\n",
      "Speed: 5.5ms preprocess, 465.8ms inference, 3.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 414.4ms\n",
      "Speed: 7.0ms preprocess, 414.4ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 604.0ms\n",
      "Speed: 7.4ms preprocess, 604.0ms inference, 5.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x448 1 person, 325.2ms\n",
      "Speed: 6.8ms preprocess, 325.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 309.2ms\n",
      "Speed: 3.0ms preprocess, 309.2ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 (no detections), 521.0ms\n",
      "Speed: 6.9ms preprocess, 521.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 309.5ms\n",
      "Speed: 6.9ms preprocess, 309.5ms inference, 5.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 669.4ms\n",
      "Speed: 9.7ms preprocess, 669.4ms inference, 7.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 persons, 276.9ms\n",
      "Speed: 4.9ms preprocess, 276.9ms inference, 4.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 311.5ms\n",
      "Speed: 6.0ms preprocess, 311.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 278.5ms\n",
      "Speed: 3.5ms preprocess, 278.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 504.7ms\n",
      "Speed: 3.5ms preprocess, 504.7ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 344.0ms\n",
      "Speed: 6.1ms preprocess, 344.0ms inference, 6.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 394.4ms\n",
      "Speed: 3.5ms preprocess, 394.4ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 335.2ms\n",
      "Speed: 9.3ms preprocess, 335.2ms inference, 4.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 531.1ms\n",
      "Speed: 4.0ms preprocess, 531.1ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 1 person, 407.7ms\n",
      "Speed: 8.1ms preprocess, 407.7ms inference, 5.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 1 person, 520.3ms\n",
      "Speed: 4.9ms preprocess, 520.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 302.8ms\n",
      "Speed: 6.2ms preprocess, 302.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 301.5ms\n",
      "Speed: 3.4ms preprocess, 301.5ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 411.9ms\n",
      "Speed: 7.1ms preprocess, 411.9ms inference, 10.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 396.8ms\n",
      "Speed: 7.0ms preprocess, 396.8ms inference, 11.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 352x640 1 person, 354.5ms\n",
      "Speed: 13.5ms preprocess, 354.5ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 person, 342.6ms\n",
      "Speed: 7.0ms preprocess, 342.6ms inference, 11.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 433.5ms\n",
      "Speed: 7.4ms preprocess, 433.5ms inference, 107.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 329.8ms\n",
      "Speed: 6.0ms preprocess, 329.8ms inference, 10.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 313.1ms\n",
      "Speed: 7.8ms preprocess, 313.1ms inference, 171.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 352x640 1 person, 344.6ms\n",
      "Speed: 8.0ms preprocess, 344.6ms inference, 16.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 343.6ms\n",
      "Speed: 5.0ms preprocess, 343.6ms inference, 16.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 382.9ms\n",
      "Speed: 7.9ms preprocess, 382.9ms inference, 10.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 545.4ms\n",
      "Speed: 7.3ms preprocess, 545.4ms inference, 62.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 544x640 1 person, 669.2ms\n",
      "Speed: 25.8ms preprocess, 669.2ms inference, 21.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x480 1 person, 408.1ms\n",
      "Speed: 15.0ms preprocess, 408.1ms inference, 11.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x544 1 person, 717.0ms\n",
      "Speed: 28.6ms preprocess, 717.0ms inference, 18.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 1 person, 873.1ms\n",
      "Speed: 15.2ms preprocess, 873.1ms inference, 16.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 278.3ms\n",
      "Speed: 9.1ms preprocess, 278.3ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 (no detections), 754.6ms\n",
      "Speed: 12.4ms preprocess, 754.6ms inference, 10.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 320x640 (no detections), 292.3ms\n",
      "Speed: 9.0ms preprocess, 292.3ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 295.6ms\n",
      "Speed: 14.5ms preprocess, 295.6ms inference, 25.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 578.3ms\n",
      "Speed: 11.1ms preprocess, 578.3ms inference, 33.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 358.4ms\n",
      "Speed: 8.4ms preprocess, 358.4ms inference, 21.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 922.6ms\n",
      "Speed: 12.0ms preprocess, 922.6ms inference, 26.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 355.0ms\n",
      "Speed: 13.6ms preprocess, 355.0ms inference, 19.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 645.2ms\n",
      "Speed: 33.0ms preprocess, 645.2ms inference, 22.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 469.5ms\n",
      "Speed: 10.7ms preprocess, 469.5ms inference, 75.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 583.7ms\n",
      "Speed: 17.0ms preprocess, 583.7ms inference, 30.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 583.0ms\n",
      "Speed: 32.9ms preprocess, 583.0ms inference, 35.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 322.4ms\n",
      "Speed: 20.5ms preprocess, 322.4ms inference, 32.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 341.8ms\n",
      "Speed: 15.4ms preprocess, 341.8ms inference, 23.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 412.9ms\n",
      "Speed: 6.1ms preprocess, 412.9ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 320x640 1 person, 515.3ms\n",
      "Speed: 30.1ms preprocess, 515.3ms inference, 39.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 384x640 1 person, 450.7ms\n",
      "Speed: 20.4ms preprocess, 450.7ms inference, 29.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 375.8ms\n",
      "Speed: 14.4ms preprocess, 375.8ms inference, 143.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 231.6ms\n",
      "Speed: 14.2ms preprocess, 231.6ms inference, 24.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 683.2ms\n",
      "Speed: 7.8ms preprocess, 683.2ms inference, 23.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 286.5ms\n",
      "Speed: 16.1ms preprocess, 286.5ms inference, 25.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 268.1ms\n",
      "Speed: 13.9ms preprocess, 268.1ms inference, 24.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 1321.9ms\n",
      "Speed: 8.5ms preprocess, 1321.9ms inference, 21.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 320x640 1 person, 487.7ms\n",
      "Speed: 28.6ms preprocess, 487.7ms inference, 43.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 384x640 1 person, 253.8ms\n",
      "Speed: 15.2ms preprocess, 253.8ms inference, 48.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 532.5ms\n",
      "Speed: 13.8ms preprocess, 532.5ms inference, 35.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 399.7ms\n",
      "Speed: 16.7ms preprocess, 399.7ms inference, 32.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 329.1ms\n",
      "Speed: 32.4ms preprocess, 329.1ms inference, 32.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.3ms\n",
      "Speed: 17.8ms preprocess, 373.3ms inference, 56.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 (no detections), 513.2ms\n",
      "Speed: 14.6ms preprocess, 513.2ms inference, 4.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 280.8ms\n",
      "Speed: 11.3ms preprocess, 280.8ms inference, 16.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 344.9ms\n",
      "Speed: 14.5ms preprocess, 344.9ms inference, 11.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 320x640 1 person, 330.0ms\n",
      "Speed: 7.1ms preprocess, 330.0ms inference, 8.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 352x640 1 person, 290.5ms\n",
      "Speed: 6.3ms preprocess, 290.5ms inference, 10.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 329.5ms\n",
      "Speed: 15.0ms preprocess, 329.5ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 323.2ms\n",
      "Speed: 5.2ms preprocess, 323.2ms inference, 7.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 305.6ms\n",
      "Speed: 3.8ms preprocess, 305.6ms inference, 15.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 352x640 1 person, 261.9ms\n",
      "Speed: 6.5ms preprocess, 261.9ms inference, 24.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x480 1 person, 442.6ms\n",
      "Speed: 42.9ms preprocess, 442.6ms inference, 24.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 512x640 1 person, 338.0ms\n",
      "Speed: 14.7ms preprocess, 338.0ms inference, 31.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 672.7ms\n",
      "Speed: 36.1ms preprocess, 672.7ms inference, 58.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 271.2ms\n",
      "Speed: 15.3ms preprocess, 271.2ms inference, 18.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 830.9ms\n",
      "Speed: 66.5ms preprocess, 830.9ms inference, 49.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 506.7ms\n",
      "Speed: 34.1ms preprocess, 506.7ms inference, 35.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 450.9ms\n",
      "Speed: 41.8ms preprocess, 450.9ms inference, 47.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 485.2ms\n",
      "Speed: 25.9ms preprocess, 485.2ms inference, 30.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 416x640 1 person, 328.2ms\n",
      "Speed: 14.6ms preprocess, 328.2ms inference, 19.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 448.6ms\n",
      "Speed: 20.4ms preprocess, 448.6ms inference, 21.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 321.5ms\n",
      "Speed: 13.5ms preprocess, 321.5ms inference, 23.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 729.2ms\n",
      "Speed: 50.6ms preprocess, 729.2ms inference, 19.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 418.0ms\n",
      "Speed: 19.7ms preprocess, 418.0ms inference, 33.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 997.8ms\n",
      "Speed: 14.0ms preprocess, 997.8ms inference, 76.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 321.2ms\n",
      "Speed: 40.7ms preprocess, 321.2ms inference, 60.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 505.3ms\n",
      "Speed: 44.1ms preprocess, 505.3ms inference, 20.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 512x640 1 person, 307.4ms\n",
      "Speed: 16.0ms preprocess, 307.4ms inference, 27.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 (no detections), 358.7ms\n",
      "Speed: 28.3ms preprocess, 358.7ms inference, 9.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 379.8ms\n",
      "Speed: 31.6ms preprocess, 379.8ms inference, 36.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 453.5ms\n",
      "Speed: 19.4ms preprocess, 453.5ms inference, 37.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 480.8ms\n",
      "Speed: 19.8ms preprocess, 480.8ms inference, 33.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 447.9ms\n",
      "Speed: 18.2ms preprocess, 447.9ms inference, 29.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 352.2ms\n",
      "Speed: 28.3ms preprocess, 352.2ms inference, 35.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 601.2ms\n",
      "Speed: 12.0ms preprocess, 601.2ms inference, 21.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 236.9ms\n",
      "Speed: 17.5ms preprocess, 236.9ms inference, 16.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 person, 300.6ms\n",
      "Speed: 17.9ms preprocess, 300.6ms inference, 17.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 303.3ms\n",
      "Speed: 9.7ms preprocess, 303.3ms inference, 14.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 (no detections), 439.8ms\n",
      "Speed: 7.0ms preprocess, 439.8ms inference, 7.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 271.0ms\n",
      "Speed: 13.5ms preprocess, 271.0ms inference, 13.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 724.1ms\n",
      "Speed: 9.5ms preprocess, 724.1ms inference, 8.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 281.3ms\n",
      "Speed: 7.0ms preprocess, 281.3ms inference, 13.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 265.7ms\n",
      "Speed: 6.0ms preprocess, 265.7ms inference, 18.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 627.5ms\n",
      "Speed: 15.2ms preprocess, 627.5ms inference, 215.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 2 persons, 635.5ms\n",
      "Speed: 72.9ms preprocess, 635.5ms inference, 25.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 625.9ms\n",
      "Speed: 93.8ms preprocess, 625.9ms inference, 40.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 334.5ms\n",
      "Speed: 16.8ms preprocess, 334.5ms inference, 20.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 764.2ms\n",
      "Speed: 14.6ms preprocess, 764.2ms inference, 19.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 494.8ms\n",
      "Speed: 13.9ms preprocess, 494.8ms inference, 38.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 255.6ms\n",
      "Speed: 12.1ms preprocess, 255.6ms inference, 13.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 260.0ms\n",
      "Speed: 11.7ms preprocess, 260.0ms inference, 15.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x544 1 person, 501.2ms\n",
      "Speed: 18.9ms preprocess, 501.2ms inference, 48.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 385.1ms\n",
      "Speed: 31.8ms preprocess, 385.1ms inference, 27.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 283.4ms\n",
      "Speed: 14.8ms preprocess, 283.4ms inference, 17.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 (no detections), 298.2ms\n",
      "Speed: 10.5ms preprocess, 298.2ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 384x640 1 person, 298.0ms\n",
      "Speed: 8.0ms preprocess, 298.0ms inference, 9.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 329.0ms\n",
      "Speed: 5.4ms preprocess, 329.0ms inference, 27.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 556.7ms\n",
      "Speed: 13.2ms preprocess, 556.7ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 427.9ms\n",
      "Speed: 123.8ms preprocess, 427.9ms inference, 25.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 636.7ms\n",
      "Speed: 22.3ms preprocess, 636.7ms inference, 21.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 322.5ms\n",
      "Speed: 12.0ms preprocess, 322.5ms inference, 61.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 599.8ms\n",
      "Speed: 16.2ms preprocess, 599.8ms inference, 17.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 person, 365.9ms\n",
      "Speed: 15.8ms preprocess, 365.9ms inference, 16.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 701.4ms\n",
      "Speed: 12.7ms preprocess, 701.4ms inference, 49.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 300.2ms\n",
      "Speed: 76.8ms preprocess, 300.2ms inference, 49.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 611.6ms\n",
      "Speed: 12.1ms preprocess, 611.6ms inference, 30.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 389.7ms\n",
      "Speed: 15.9ms preprocess, 389.7ms inference, 28.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 718.8ms\n",
      "Speed: 29.5ms preprocess, 718.8ms inference, 117.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 368.4ms\n",
      "Speed: 123.0ms preprocess, 368.4ms inference, 54.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 401.8ms\n",
      "Speed: 119.0ms preprocess, 401.8ms inference, 35.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 2702.2ms\n",
      "Speed: 25.5ms preprocess, 2702.2ms inference, 109.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 445.8ms\n",
      "Speed: 37.1ms preprocess, 445.8ms inference, 21.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 785.1ms\n",
      "Speed: 29.0ms preprocess, 785.1ms inference, 221.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 498.3ms\n",
      "Speed: 16.5ms preprocess, 498.3ms inference, 40.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x384 3 persons, 595.1ms\n",
      "Speed: 13.1ms preprocess, 595.1ms inference, 26.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x640 1 person, 489.9ms\n",
      "Speed: 12.2ms preprocess, 489.9ms inference, 40.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 380.2ms\n",
      "Speed: 32.1ms preprocess, 380.2ms inference, 27.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 467.0ms\n",
      "Speed: 12.1ms preprocess, 467.0ms inference, 72.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 365.5ms\n",
      "Speed: 11.6ms preprocess, 365.5ms inference, 22.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 574.6ms\n",
      "Speed: 38.7ms preprocess, 574.6ms inference, 32.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 322.9ms\n",
      "Speed: 12.6ms preprocess, 322.9ms inference, 77.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 728.1ms\n",
      "Speed: 35.8ms preprocess, 728.1ms inference, 70.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x480 1 person, 295.4ms\n",
      "Speed: 10.2ms preprocess, 295.4ms inference, 22.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 1060.6ms\n",
      "Speed: 15.1ms preprocess, 1060.6ms inference, 76.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 2470.6ms\n",
      "Speed: 246.0ms preprocess, 2470.6ms inference, 61.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 423.1ms\n",
      "Speed: 16.4ms preprocess, 423.1ms inference, 31.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 891.6ms\n",
      "Speed: 19.4ms preprocess, 891.6ms inference, 258.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 352x640 1 person, 1151.2ms\n",
      "Speed: 55.4ms preprocess, 1151.2ms inference, 203.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 1199.6ms\n",
      "Speed: 58.1ms preprocess, 1199.6ms inference, 109.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 527.4ms\n",
      "Speed: 35.6ms preprocess, 527.4ms inference, 28.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 562.6ms\n",
      "Speed: 17.1ms preprocess, 562.6ms inference, 89.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 433.3ms\n",
      "Speed: 29.4ms preprocess, 433.3ms inference, 65.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 1 person, 547.1ms\n",
      "Speed: 33.6ms preprocess, 547.1ms inference, 19.9ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 person, 383.5ms\n",
      "Speed: 12.1ms preprocess, 383.5ms inference, 32.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 531.2ms\n",
      "Speed: 24.2ms preprocess, 531.2ms inference, 10.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 470.8ms\n",
      "Speed: 20.0ms preprocess, 470.8ms inference, 74.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 339.2ms\n",
      "Speed: 16.8ms preprocess, 339.2ms inference, 16.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 780.4ms\n",
      "Speed: 26.9ms preprocess, 780.4ms inference, 70.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 333.5ms\n",
      "Speed: 17.4ms preprocess, 333.5ms inference, 28.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 491.2ms\n",
      "Speed: 8.8ms preprocess, 491.2ms inference, 34.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 1 person, 673.7ms\n",
      "Speed: 24.5ms preprocess, 673.7ms inference, 159.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x640 1 person, 1946.8ms\n",
      "Speed: 150.8ms preprocess, 1946.8ms inference, 48.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1369.8ms\n",
      "Speed: 38.2ms preprocess, 1369.8ms inference, 233.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1639.9ms\n",
      "Speed: 43.6ms preprocess, 1639.9ms inference, 164.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 270.0ms\n",
      "Speed: 33.0ms preprocess, 270.0ms inference, 12.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 288x640 1 person, 1010.0ms\n",
      "Speed: 5.0ms preprocess, 1010.0ms inference, 151.5ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 448x640 1 person, 1397.5ms\n",
      "Speed: 26.8ms preprocess, 1397.5ms inference, 131.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1147.0ms\n",
      "Speed: 31.8ms preprocess, 1147.0ms inference, 56.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x544 2 persons, 1517.3ms\n",
      "Speed: 49.8ms preprocess, 1517.3ms inference, 45.7ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 725.9ms\n",
      "Speed: 50.7ms preprocess, 725.9ms inference, 88.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1582.5ms\n",
      "Speed: 53.2ms preprocess, 1582.5ms inference, 77.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 275.8ms\n",
      "Speed: 25.9ms preprocess, 275.8ms inference, 29.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 2 persons, 710.9ms\n",
      "Speed: 78.3ms preprocess, 710.9ms inference, 53.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 469.0ms\n",
      "Speed: 18.7ms preprocess, 469.0ms inference, 36.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x256 2 persons, 503.0ms\n",
      "Speed: 31.1ms preprocess, 503.0ms inference, 45.4ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 448x640 1 person, 347.3ms\n",
      "Speed: 30.4ms preprocess, 347.3ms inference, 32.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1018.2ms\n",
      "Speed: 30.1ms preprocess, 1018.2ms inference, 26.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 353.0ms\n",
      "Speed: 14.3ms preprocess, 353.0ms inference, 34.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1199.6ms\n",
      "Speed: 38.6ms preprocess, 1199.6ms inference, 308.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1052.1ms\n",
      "Speed: 42.2ms preprocess, 1052.1ms inference, 119.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x576 1 person, 816.5ms\n",
      "Speed: 49.2ms preprocess, 816.5ms inference, 60.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 1 person, 947.0ms\n",
      "Speed: 20.6ms preprocess, 947.0ms inference, 63.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 person, 721.0ms\n",
      "Speed: 41.5ms preprocess, 721.0ms inference, 70.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 448x640 1 person, 366.3ms\n",
      "Speed: 18.5ms preprocess, 366.3ms inference, 56.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 381.1ms\n",
      "Speed: 35.0ms preprocess, 381.1ms inference, 23.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 701.3ms\n",
      "Speed: 38.2ms preprocess, 701.3ms inference, 48.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1749.8ms\n",
      "Speed: 10.6ms preprocess, 1749.8ms inference, 281.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 1616.8ms\n",
      "Speed: 231.9ms preprocess, 1616.8ms inference, 27.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 352x640 1 person, 1821.2ms\n",
      "Speed: 23.0ms preprocess, 1821.2ms inference, 318.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1703.2ms\n",
      "Speed: 20.1ms preprocess, 1703.2ms inference, 34.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1014.3ms\n",
      "Speed: 65.9ms preprocess, 1014.3ms inference, 63.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x576 1 person, 1306.7ms\n",
      "Speed: 30.5ms preprocess, 1306.7ms inference, 113.6ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 480x640 1 person, 728.1ms\n",
      "Speed: 148.9ms preprocess, 728.1ms inference, 55.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 539.9ms\n",
      "Speed: 30.3ms preprocess, 539.9ms inference, 42.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 2551.4ms\n",
      "Speed: 33.8ms preprocess, 2551.4ms inference, 186.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 3 persons, 951.8ms\n",
      "Speed: 26.1ms preprocess, 951.8ms inference, 30.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1280.8ms\n",
      "Speed: 25.2ms preprocess, 1280.8ms inference, 56.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 873.5ms\n",
      "Speed: 25.5ms preprocess, 873.5ms inference, 97.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 676.4ms\n",
      "Speed: 26.8ms preprocess, 676.4ms inference, 148.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.8ms\n",
      "Speed: 15.7ms preprocess, 396.8ms inference, 74.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 594.9ms\n",
      "Speed: 27.7ms preprocess, 594.9ms inference, 33.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 3 persons, 1018.2ms\n",
      "Speed: 30.2ms preprocess, 1018.2ms inference, 160.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 352x640 1 person, 1421.7ms\n",
      "Speed: 18.4ms preprocess, 1421.7ms inference, 95.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x448 1 person, 883.8ms\n",
      "Speed: 22.6ms preprocess, 883.8ms inference, 37.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1855.3ms\n",
      "Speed: 14.9ms preprocess, 1855.3ms inference, 127.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 224x640 2 persons, 1483.8ms\n",
      "Speed: 13.1ms preprocess, 1483.8ms inference, 59.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 448x640 1 person, 1211.4ms\n",
      "Speed: 20.4ms preprocess, 1211.4ms inference, 58.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 905.9ms\n",
      "Speed: 14.1ms preprocess, 905.9ms inference, 118.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1387.5ms\n",
      "Speed: 252.1ms preprocess, 1387.5ms inference, 149.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 844.8ms\n",
      "Speed: 19.4ms preprocess, 844.8ms inference, 45.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 531.2ms\n",
      "Speed: 18.0ms preprocess, 531.2ms inference, 114.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 2 persons, 844.8ms\n",
      "Speed: 35.3ms preprocess, 844.8ms inference, 29.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 1277.0ms\n",
      "Speed: 51.3ms preprocess, 1277.0ms inference, 92.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 1030.6ms\n",
      "Speed: 50.6ms preprocess, 1030.6ms inference, 39.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 501.9ms\n",
      "Speed: 101.6ms preprocess, 501.9ms inference, 69.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 488.0ms\n",
      "Speed: 13.8ms preprocess, 488.0ms inference, 45.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 516.4ms\n",
      "Speed: 49.9ms preprocess, 516.4ms inference, 67.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 652.9ms\n",
      "Speed: 31.8ms preprocess, 652.9ms inference, 48.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 553.0ms\n",
      "Speed: 14.9ms preprocess, 553.0ms inference, 39.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 695.8ms\n",
      "Speed: 18.1ms preprocess, 695.8ms inference, 49.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 376.5ms\n",
      "Speed: 26.7ms preprocess, 376.5ms inference, 51.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 530.1ms\n",
      "Speed: 25.2ms preprocess, 530.1ms inference, 26.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 902.0ms\n",
      "Speed: 22.6ms preprocess, 902.0ms inference, 127.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1558.3ms\n",
      "Speed: 24.2ms preprocess, 1558.3ms inference, 197.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 762.4ms\n",
      "Speed: 112.5ms preprocess, 762.4ms inference, 54.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 856.8ms\n",
      "Speed: 23.5ms preprocess, 856.8ms inference, 185.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1368.1ms\n",
      "Speed: 27.4ms preprocess, 1368.1ms inference, 349.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2029.5ms\n",
      "Speed: 16.7ms preprocess, 2029.5ms inference, 500.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 1163.4ms\n",
      "Speed: 233.1ms preprocess, 1163.4ms inference, 135.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 2474.8ms\n",
      "Speed: 104.6ms preprocess, 2474.8ms inference, 385.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 433.0ms\n",
      "Speed: 114.2ms preprocess, 433.0ms inference, 120.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1079.3ms\n",
      "Speed: 12.8ms preprocess, 1079.3ms inference, 216.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1468.5ms\n",
      "Speed: 167.0ms preprocess, 1468.5ms inference, 58.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 3078.6ms\n",
      "Speed: 143.3ms preprocess, 3078.6ms inference, 190.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1235.3ms\n",
      "Speed: 232.1ms preprocess, 1235.3ms inference, 70.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 433.6ms\n",
      "Speed: 33.7ms preprocess, 433.6ms inference, 65.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 636.5ms\n",
      "Speed: 25.2ms preprocess, 636.5ms inference, 37.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 466.6ms\n",
      "Speed: 31.6ms preprocess, 466.6ms inference, 44.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1550.0ms\n",
      "Speed: 22.3ms preprocess, 1550.0ms inference, 495.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 2 persons, 642.2ms\n",
      "Speed: 25.3ms preprocess, 642.2ms inference, 28.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 916.6ms\n",
      "Speed: 54.5ms preprocess, 916.6ms inference, 81.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1683.6ms\n",
      "Speed: 248.2ms preprocess, 1683.6ms inference, 209.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 306.0ms\n",
      "Speed: 35.8ms preprocess, 306.0ms inference, 13.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 1019.2ms\n",
      "Speed: 7.8ms preprocess, 1019.2ms inference, 10.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 352.1ms\n",
      "Speed: 39.2ms preprocess, 352.1ms inference, 22.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 (no detections), 835.0ms\n",
      "Speed: 9.9ms preprocess, 835.0ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 639.4ms\n",
      "Speed: 13.4ms preprocess, 639.4ms inference, 18.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 361.7ms\n",
      "Speed: 7.2ms preprocess, 361.7ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 682.2ms\n",
      "Speed: 12.8ms preprocess, 682.2ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 409.0ms\n",
      "Speed: 6.0ms preprocess, 409.0ms inference, 10.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 512x640 1 person, 582.0ms\n",
      "Speed: 9.7ms preprocess, 582.0ms inference, 6.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 905.4ms\n",
      "Speed: 8.8ms preprocess, 905.4ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 955.9ms\n",
      "Speed: 5.5ms preprocess, 955.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 969.1ms\n",
      "Speed: 6.2ms preprocess, 969.1ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 823.1ms\n",
      "Speed: 5.0ms preprocess, 823.1ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 748.2ms\n",
      "Speed: 5.3ms preprocess, 748.2ms inference, 5.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 1089.5ms\n",
      "Speed: 5.5ms preprocess, 1089.5ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 350.3ms\n",
      "Speed: 5.0ms preprocess, 350.3ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1036.4ms\n",
      "Speed: 3.4ms preprocess, 1036.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 929.9ms\n",
      "Speed: 7.7ms preprocess, 929.9ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 1 person, 407.5ms\n",
      "Speed: 4.5ms preprocess, 407.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 (no detections), 1058.1ms\n",
      "Speed: 7.2ms preprocess, 1058.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 658.1ms\n",
      "Speed: 5.6ms preprocess, 658.1ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 551.2ms\n",
      "Speed: 3.4ms preprocess, 551.2ms inference, 93.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 person, 499.0ms\n",
      "Speed: 11.5ms preprocess, 499.0ms inference, 120.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1241.7ms\n",
      "Speed: 31.0ms preprocess, 1241.7ms inference, 14.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 313.7ms\n",
      "Speed: 16.9ms preprocess, 313.7ms inference, 11.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 358.0ms\n",
      "Speed: 11.2ms preprocess, 358.0ms inference, 8.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x384 1 person, 496.7ms\n",
      "Speed: 4.1ms preprocess, 496.7ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 512x640 9 persons, 766.0ms\n",
      "Speed: 7.0ms preprocess, 766.0ms inference, 8.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 523.8ms\n",
      "Speed: 8.2ms preprocess, 523.8ms inference, 6.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 352x640 1 person, 709.1ms\n",
      "Speed: 5.4ms preprocess, 709.1ms inference, 6.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 308.2ms\n",
      "Speed: 8.4ms preprocess, 308.2ms inference, 7.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 838.3ms\n",
      "Speed: 7.0ms preprocess, 838.3ms inference, 8.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 person, 443.2ms\n",
      "Speed: 4.5ms preprocess, 443.2ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 353.3ms\n",
      "Speed: 6.6ms preprocess, 353.3ms inference, 5.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x384 1 person, 318.1ms\n",
      "Speed: 2.5ms preprocess, 318.1ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 480x640 1 person, 729.5ms\n",
      "Speed: 9.5ms preprocess, 729.5ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 6 persons, 1076.9ms\n",
      "Speed: 10.5ms preprocess, 1076.9ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 384.9ms\n",
      "Speed: 11.2ms preprocess, 384.9ms inference, 12.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x448 1 person, 886.9ms\n",
      "Speed: 10.3ms preprocess, 886.9ms inference, 24.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 629.8ms\n",
      "Speed: 35.3ms preprocess, 629.8ms inference, 24.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 616.8ms\n",
      "Speed: 17.0ms preprocess, 616.8ms inference, 17.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 667.3ms\n",
      "Speed: 10.2ms preprocess, 667.3ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1096.9ms\n",
      "Speed: 10.4ms preprocess, 1096.9ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 869.8ms\n",
      "Speed: 13.1ms preprocess, 869.8ms inference, 18.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 829.1ms\n",
      "Speed: 10.3ms preprocess, 829.1ms inference, 31.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 549.1ms\n",
      "Speed: 10.7ms preprocess, 549.1ms inference, 9.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 857.5ms\n",
      "Speed: 10.5ms preprocess, 857.5ms inference, 11.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 838.7ms\n",
      "Speed: 9.5ms preprocess, 838.7ms inference, 11.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 752.7ms\n",
      "Speed: 8.1ms preprocess, 752.7ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 921.5ms\n",
      "Speed: 9.4ms preprocess, 921.5ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 792.8ms\n",
      "Speed: 5.5ms preprocess, 792.8ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 384.8ms\n",
      "Speed: 8.8ms preprocess, 384.8ms inference, 10.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 881.8ms\n",
      "Speed: 6.0ms preprocess, 881.8ms inference, 10.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 659.8ms\n",
      "Speed: 6.6ms preprocess, 659.8ms inference, 12.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 657.3ms\n",
      "Speed: 6.2ms preprocess, 657.3ms inference, 10.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x512 1 person, 1302.8ms\n",
      "Speed: 12.5ms preprocess, 1302.8ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x448 1 person, 329.8ms\n",
      "Speed: 6.6ms preprocess, 329.8ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 2 persons, 1077.6ms\n",
      "Speed: 8.5ms preprocess, 1077.6ms inference, 10.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1150.3ms\n",
      "Speed: 4.9ms preprocess, 1150.3ms inference, 12.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 354.3ms\n",
      "Speed: 15.2ms preprocess, 354.3ms inference, 16.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 1082.0ms\n",
      "Speed: 9.2ms preprocess, 1082.0ms inference, 11.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 959.3ms\n",
      "Speed: 8.3ms preprocess, 959.3ms inference, 13.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x480 1 person, 337.7ms\n",
      "Speed: 8.1ms preprocess, 337.7ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 1308.6ms\n",
      "Speed: 7.4ms preprocess, 1308.6ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 975.6ms\n",
      "Speed: 9.8ms preprocess, 975.6ms inference, 10.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 812.0ms\n",
      "Speed: 10.7ms preprocess, 812.0ms inference, 12.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 505.3ms\n",
      "Speed: 13.6ms preprocess, 505.3ms inference, 8.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 576x640 1 person, 1126.6ms\n",
      "Speed: 13.5ms preprocess, 1126.6ms inference, 10.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x512 1 person, 906.8ms\n",
      "Speed: 9.8ms preprocess, 906.8ms inference, 15.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 416x640 1 person, 510.3ms\n",
      "Speed: 8.7ms preprocess, 510.3ms inference, 13.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 847.4ms\n",
      "Speed: 15.3ms preprocess, 847.4ms inference, 31.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 650.4ms\n",
      "Speed: 19.6ms preprocess, 650.4ms inference, 12.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 558.9ms\n",
      "Speed: 6.0ms preprocess, 558.9ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 557.8ms\n",
      "Speed: 8.1ms preprocess, 557.8ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 982.1ms\n",
      "Speed: 13.0ms preprocess, 982.1ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 932.0ms\n",
      "Speed: 6.5ms preprocess, 932.0ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 256x640 1 person, 360.6ms\n",
      "Speed: 11.6ms preprocess, 360.6ms inference, 22.4ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 448x640 1 person, 457.1ms\n",
      "Speed: 21.0ms preprocess, 457.1ms inference, 13.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 686.2ms\n",
      "Speed: 10.3ms preprocess, 686.2ms inference, 13.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1118.3ms\n",
      "Speed: 12.7ms preprocess, 1118.3ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1083.4ms\n",
      "Speed: 13.8ms preprocess, 1083.4ms inference, 38.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 767.4ms\n",
      "Speed: 21.7ms preprocess, 767.4ms inference, 38.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 304.4ms\n",
      "Speed: 10.8ms preprocess, 304.4ms inference, 50.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1101.3ms\n",
      "Speed: 12.2ms preprocess, 1101.3ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 1 person, 1175.9ms\n",
      "Speed: 9.0ms preprocess, 1175.9ms inference, 6.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 777.3ms\n",
      "Speed: 9.8ms preprocess, 777.3ms inference, 13.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 822.5ms\n",
      "Speed: 5.5ms preprocess, 822.5ms inference, 11.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 462.5ms\n",
      "Speed: 14.0ms preprocess, 462.5ms inference, 20.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1061.3ms\n",
      "Speed: 11.5ms preprocess, 1061.3ms inference, 23.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 365.0ms\n",
      "Speed: 15.9ms preprocess, 365.0ms inference, 24.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 873.9ms\n",
      "Speed: 9.6ms preprocess, 873.9ms inference, 15.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 575.5ms\n",
      "Speed: 17.4ms preprocess, 575.5ms inference, 77.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 503.2ms\n",
      "Speed: 19.1ms preprocess, 503.2ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 320x640 1 person, 410.0ms\n",
      "Speed: 9.7ms preprocess, 410.0ms inference, 19.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 1 person, 1435.0ms\n",
      "Speed: 20.6ms preprocess, 1435.0ms inference, 21.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 581.8ms\n",
      "Speed: 16.0ms preprocess, 581.8ms inference, 18.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 634.4ms\n",
      "Speed: 19.3ms preprocess, 634.4ms inference, 22.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 340.0ms\n",
      "Speed: 40.1ms preprocess, 340.0ms inference, 32.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 1151.9ms\n",
      "Speed: 24.8ms preprocess, 1151.9ms inference, 19.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 416x640 1 person, 980.8ms\n",
      "Speed: 7.9ms preprocess, 980.8ms inference, 34.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 861.5ms\n",
      "Speed: 19.8ms preprocess, 861.5ms inference, 22.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 776.0ms\n",
      "Speed: 13.1ms preprocess, 776.0ms inference, 30.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 576x640 1 person, 980.3ms\n",
      "Speed: 11.9ms preprocess, 980.3ms inference, 8.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 1 person, 863.6ms\n",
      "Speed: 9.8ms preprocess, 863.6ms inference, 33.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 378.8ms\n",
      "Speed: 14.9ms preprocess, 378.8ms inference, 8.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 973.1ms\n",
      "Speed: 10.1ms preprocess, 973.1ms inference, 11.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 917.6ms\n",
      "Speed: 10.9ms preprocess, 917.6ms inference, 19.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 1364.8ms\n",
      "Speed: 25.8ms preprocess, 1364.8ms inference, 23.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1032.0ms\n",
      "Speed: 14.7ms preprocess, 1032.0ms inference, 63.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 337.4ms\n",
      "Speed: 20.4ms preprocess, 337.4ms inference, 26.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 1 person, 1149.3ms\n",
      "Speed: 34.3ms preprocess, 1149.3ms inference, 19.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 person, 896.2ms\n",
      "Speed: 25.3ms preprocess, 896.2ms inference, 20.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1523.9ms\n",
      "Speed: 14.1ms preprocess, 1523.9ms inference, 19.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 1 person, 1012.7ms\n",
      "Speed: 12.6ms preprocess, 1012.7ms inference, 16.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 1 person, 809.9ms\n",
      "Speed: 15.3ms preprocess, 809.9ms inference, 21.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 721.7ms\n",
      "Speed: 10.1ms preprocess, 721.7ms inference, 43.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 344.9ms\n",
      "Speed: 16.6ms preprocess, 344.9ms inference, 16.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1073.0ms\n",
      "Speed: 10.7ms preprocess, 1073.0ms inference, 18.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 337.6ms\n",
      "Speed: 9.6ms preprocess, 337.6ms inference, 28.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 1048.8ms\n",
      "Speed: 22.6ms preprocess, 1048.8ms inference, 19.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 1 person, 1007.1ms\n",
      "Speed: 16.2ms preprocess, 1007.1ms inference, 14.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 1 person, 622.7ms\n",
      "Speed: 6.5ms preprocess, 622.7ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 660.9ms\n",
      "Speed: 5.3ms preprocess, 660.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 839.0ms\n",
      "Speed: 8.1ms preprocess, 839.0ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 750.3ms\n",
      "Speed: 6.0ms preprocess, 750.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 746.3ms\n",
      "Speed: 4.8ms preprocess, 746.3ms inference, 6.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 778.1ms\n",
      "Speed: 4.0ms preprocess, 778.1ms inference, 3.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 747.9ms\n",
      "Speed: 3.9ms preprocess, 747.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 785.2ms\n",
      "Speed: 5.6ms preprocess, 785.2ms inference, 5.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 751.7ms\n",
      "Speed: 5.5ms preprocess, 751.7ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 888.3ms\n",
      "Speed: 7.0ms preprocess, 888.3ms inference, 6.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 755.5ms\n",
      "Speed: 5.9ms preprocess, 755.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 763.5ms\n",
      "Speed: 5.0ms preprocess, 763.5ms inference, 5.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 1 person, 1062.5ms\n",
      "Speed: 18.4ms preprocess, 1062.5ms inference, 5.9ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 person, 1107.9ms\n",
      "Speed: 9.6ms preprocess, 1107.9ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 705.4ms\n",
      "Speed: 3.7ms preprocess, 705.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 704.0ms\n",
      "Speed: 2.5ms preprocess, 704.0ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 843.7ms\n",
      "Speed: 7.7ms preprocess, 843.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 659.0ms\n",
      "Speed: 5.3ms preprocess, 659.0ms inference, 5.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 798.9ms\n",
      "Speed: 7.5ms preprocess, 798.9ms inference, 5.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 693.3ms\n",
      "Speed: 5.9ms preprocess, 693.3ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 824.7ms\n",
      "Speed: 6.5ms preprocess, 824.7ms inference, 18.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1265.9ms\n",
      "Speed: 74.1ms preprocess, 1265.9ms inference, 36.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 334.6ms\n",
      "Speed: 31.2ms preprocess, 334.6ms inference, 26.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 712.9ms\n",
      "Speed: 39.7ms preprocess, 712.9ms inference, 14.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 1 person, 1547.5ms\n",
      "Speed: 15.4ms preprocess, 1547.5ms inference, 9.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1530.7ms\n",
      "Speed: 6.9ms preprocess, 1530.7ms inference, 13.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 780.9ms\n",
      "Speed: 10.6ms preprocess, 780.9ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 733.2ms\n",
      "Speed: 6.0ms preprocess, 733.2ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 809.8ms\n",
      "Speed: 8.8ms preprocess, 809.8ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 693.7ms\n",
      "Speed: 9.0ms preprocess, 693.7ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1419.4ms\n",
      "Speed: 6.3ms preprocess, 1419.4ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 451.7ms\n",
      "Speed: 6.8ms preprocess, 451.7ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 1276.7ms\n",
      "Speed: 9.1ms preprocess, 1276.7ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1183.9ms\n",
      "Speed: 7.2ms preprocess, 1183.9ms inference, 29.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 person, 998.8ms\n",
      "Speed: 13.6ms preprocess, 998.8ms inference, 16.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 320x640 1 person, 523.9ms\n",
      "Speed: 17.6ms preprocess, 523.9ms inference, 14.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 480x640 1 person, 760.6ms\n",
      "Speed: 11.8ms preprocess, 760.6ms inference, 11.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1422.1ms\n",
      "Speed: 11.1ms preprocess, 1422.1ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1076.7ms\n",
      "Speed: 18.2ms preprocess, 1076.7ms inference, 19.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1055.5ms\n",
      "Speed: 10.4ms preprocess, 1055.5ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 574.2ms\n",
      "Speed: 5.5ms preprocess, 574.2ms inference, 63.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 794.7ms\n",
      "Speed: 6.1ms preprocess, 794.7ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1170.7ms\n",
      "Speed: 6.6ms preprocess, 1170.7ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 352x640 1 person, 390.8ms\n",
      "Speed: 8.4ms preprocess, 390.8ms inference, 91.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1542.2ms\n",
      "Speed: 15.8ms preprocess, 1542.2ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1423.1ms\n",
      "Speed: 7.7ms preprocess, 1423.1ms inference, 10.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 1 person, 854.2ms\n",
      "Speed: 9.7ms preprocess, 854.2ms inference, 10.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1402.0ms\n",
      "Speed: 7.1ms preprocess, 1402.0ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 639.5ms\n",
      "Speed: 6.8ms preprocess, 639.5ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 271.7ms\n",
      "Speed: 8.0ms preprocess, 271.7ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.5ms\n",
      "Speed: 5.0ms preprocess, 1662.5ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 1 person, 1342.0ms\n",
      "Speed: 8.6ms preprocess, 1342.0ms inference, 6.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 288x640 1 person, 690.5ms\n",
      "Speed: 5.6ms preprocess, 690.5ms inference, 4.5ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 448x640 1 person, 1806.0ms\n",
      "Speed: 7.3ms preprocess, 1806.0ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 1 person, 1248.1ms\n",
      "Speed: 14.0ms preprocess, 1248.1ms inference, 6.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 person, 1178.3ms\n",
      "Speed: 7.0ms preprocess, 1178.3ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1524.4ms\n",
      "Speed: 7.8ms preprocess, 1524.4ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 600.0ms\n",
      "Speed: 6.6ms preprocess, 600.0ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1512.4ms\n",
      "Speed: 5.0ms preprocess, 1512.4ms inference, 8.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1035.2ms\n",
      "Speed: 6.0ms preprocess, 1035.2ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 911.4ms\n",
      "Speed: 4.7ms preprocess, 911.4ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 736.4ms\n",
      "Speed: 4.6ms preprocess, 736.4ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 805.4ms\n",
      "Speed: 3.5ms preprocess, 805.4ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 852.9ms\n",
      "Speed: 5.5ms preprocess, 852.9ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 804.1ms\n",
      "Speed: 5.5ms preprocess, 804.1ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1237.4ms\n",
      "Speed: 4.9ms preprocess, 1237.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1654.8ms\n",
      "Speed: 6.1ms preprocess, 1654.8ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1023.4ms\n",
      "Speed: 8.1ms preprocess, 1023.4ms inference, 12.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 897.2ms\n",
      "Speed: 6.6ms preprocess, 897.2ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1180.3ms\n",
      "Speed: 3.5ms preprocess, 1180.3ms inference, 8.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 1555.5ms\n",
      "Speed: 5.0ms preprocess, 1555.5ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 (no detections), 1426.9ms\n",
      "Speed: 5.4ms preprocess, 1426.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 822.1ms\n",
      "Speed: 6.9ms preprocess, 822.1ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1326.3ms\n",
      "Speed: 7.5ms preprocess, 1326.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 371.8ms\n",
      "Speed: 5.0ms preprocess, 371.8ms inference, 8.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1434.0ms\n",
      "Speed: 6.5ms preprocess, 1434.0ms inference, 10.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 799.9ms\n",
      "Speed: 4.6ms preprocess, 799.9ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 787.3ms\n",
      "Speed: 4.0ms preprocess, 787.3ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 831.9ms\n",
      "Speed: 7.4ms preprocess, 831.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 1676.9ms\n",
      "Speed: 7.7ms preprocess, 1676.9ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 526.4ms\n",
      "Speed: 9.2ms preprocess, 526.4ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1519.6ms\n",
      "Speed: 9.8ms preprocess, 1519.6ms inference, 8.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1416.4ms\n",
      "Speed: 7.6ms preprocess, 1416.4ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 952.6ms\n",
      "Speed: 7.1ms preprocess, 952.6ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1419.5ms\n",
      "Speed: 5.0ms preprocess, 1419.5ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 785.8ms\n",
      "Speed: 4.6ms preprocess, 785.8ms inference, 4.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 931.1ms\n",
      "Speed: 4.9ms preprocess, 931.1ms inference, 6.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 885.5ms\n",
      "Speed: 7.8ms preprocess, 885.5ms inference, 9.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 person, 856.6ms\n",
      "Speed: 5.2ms preprocess, 856.6ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 812.6ms\n",
      "Speed: 9.0ms preprocess, 812.6ms inference, 12.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 697.7ms\n",
      "Speed: 6.7ms preprocess, 697.7ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 798.5ms\n",
      "Speed: 9.6ms preprocess, 798.5ms inference, 5.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 736.4ms\n",
      "Speed: 5.0ms preprocess, 736.4ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 288x640 1 person, 614.4ms\n",
      "Speed: 3.0ms preprocess, 614.4ms inference, 9.3ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 640x640 1 person, 1165.0ms\n",
      "Speed: 12.1ms preprocess, 1165.0ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1152.1ms\n",
      "Speed: 13.2ms preprocess, 1152.1ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 889.3ms\n",
      "Speed: 5.0ms preprocess, 889.3ms inference, 5.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 352x640 1 person, 687.3ms\n",
      "Speed: 8.3ms preprocess, 687.3ms inference, 7.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 786.4ms\n",
      "Speed: 8.7ms preprocess, 786.4ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 714.0ms\n",
      "Speed: 8.1ms preprocess, 714.0ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 606.9ms\n",
      "Speed: 4.5ms preprocess, 606.9ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1981.7ms\n",
      "Speed: 6.0ms preprocess, 1981.7ms inference, 6.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 1059.6ms\n",
      "Speed: 6.8ms preprocess, 1059.6ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 989.9ms\n",
      "Speed: 4.1ms preprocess, 989.9ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1012.6ms\n",
      "Speed: 4.7ms preprocess, 1012.6ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 288x640 1 person, 692.3ms\n",
      "Speed: 4.0ms preprocess, 692.3ms inference, 6.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 448x640 1 person, 1164.9ms\n",
      "Speed: 6.5ms preprocess, 1164.9ms inference, 7.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 2 persons, 1132.5ms\n",
      "Speed: 8.3ms preprocess, 1132.5ms inference, 8.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 352x640 1 person, 739.3ms\n",
      "Speed: 11.7ms preprocess, 739.3ms inference, 8.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 1178.8ms\n",
      "Speed: 7.4ms preprocess, 1178.8ms inference, 10.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 905.3ms\n",
      "Speed: 7.5ms preprocess, 905.3ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1194.3ms\n",
      "Speed: 6.1ms preprocess, 1194.3ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1186.9ms\n",
      "Speed: 9.6ms preprocess, 1186.9ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1189.5ms\n",
      "Speed: 9.6ms preprocess, 1189.5ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1264.2ms\n",
      "Speed: 4.6ms preprocess, 1264.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1144.6ms\n",
      "Speed: 4.0ms preprocess, 1144.6ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.4ms\n",
      "Speed: 12.4ms preprocess, 2436.4ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.0ms\n",
      "Speed: 5.0ms preprocess, 1663.0ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2435.6ms\n",
      "Speed: 10.6ms preprocess, 2435.6ms inference, 7.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.9ms\n",
      "Speed: 6.6ms preprocess, 1807.9ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.8ms\n",
      "Speed: 9.1ms preprocess, 1661.8ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2437.2ms\n",
      "Speed: 9.6ms preprocess, 2437.2ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1806.9ms\n",
      "Speed: 5.8ms preprocess, 1806.9ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.2ms\n",
      "Speed: 7.9ms preprocess, 1807.2ms inference, 5.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 1493.1ms\n",
      "Speed: 4.1ms preprocess, 1493.1ms inference, 6.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.3ms\n",
      "Speed: 7.2ms preprocess, 1983.3ms inference, 9.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 288x640 1 person, 1439.3ms\n",
      "Speed: 8.6ms preprocess, 1439.3ms inference, 5.2ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.9ms\n",
      "Speed: 6.6ms preprocess, 1808.9ms inference, 5.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.2ms\n",
      "Speed: 9.0ms preprocess, 1903.2ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2433.8ms\n",
      "Speed: 9.9ms preprocess, 2433.8ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1767.3ms\n",
      "Speed: 5.9ms preprocess, 1767.3ms inference, 6.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.8ms\n",
      "Speed: 11.5ms preprocess, 2438.8ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2435.1ms\n",
      "Speed: 9.6ms preprocess, 2435.1ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.3ms\n",
      "Speed: 7.6ms preprocess, 1982.3ms inference, 5.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.6ms\n",
      "Speed: 6.5ms preprocess, 1665.6ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.9ms\n",
      "Speed: 8.6ms preprocess, 1983.9ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.1ms\n",
      "Speed: 8.5ms preprocess, 2438.1ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1901.3ms\n",
      "Speed: 7.3ms preprocess, 1901.3ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.4ms\n",
      "Speed: 6.5ms preprocess, 1661.4ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.6ms\n",
      "Speed: 6.6ms preprocess, 1903.6ms inference, 9.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.0ms\n",
      "Speed: 8.4ms preprocess, 2437.0ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2432.9ms\n",
      "Speed: 15.7ms preprocess, 2432.9ms inference, 14.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.5ms\n",
      "Speed: 13.1ms preprocess, 1904.5ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.4ms\n",
      "Speed: 8.8ms preprocess, 2437.4ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 2 persons, 2350.1ms\n",
      "Speed: 9.0ms preprocess, 2350.1ms inference, 6.9ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 person, 2434.8ms\n",
      "Speed: 9.0ms preprocess, 2434.8ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 1 person, 2350.0ms\n",
      "Speed: 17.5ms preprocess, 2350.0ms inference, 4.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.2ms\n",
      "Speed: 6.2ms preprocess, 1664.2ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1660.2ms\n",
      "Speed: 6.0ms preprocess, 1660.2ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.1ms\n",
      "Speed: 15.0ms preprocess, 1661.1ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2436.6ms\n",
      "Speed: 12.2ms preprocess, 2436.6ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2435.7ms\n",
      "Speed: 9.1ms preprocess, 2435.7ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1668.7ms\n",
      "Speed: 5.5ms preprocess, 1668.7ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1812.9ms\n",
      "Speed: 6.1ms preprocess, 1812.9ms inference, 10.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1671.2ms\n",
      "Speed: 7.1ms preprocess, 1671.2ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.4ms\n",
      "Speed: 9.3ms preprocess, 1810.4ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.7ms\n",
      "Speed: 8.1ms preprocess, 1903.7ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1900.8ms\n",
      "Speed: 8.1ms preprocess, 1900.8ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.5ms\n",
      "Speed: 5.4ms preprocess, 1663.5ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.0ms\n",
      "Speed: 7.2ms preprocess, 1662.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.5ms\n",
      "Speed: 6.0ms preprocess, 1812.5ms inference, 6.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.0ms\n",
      "Speed: 10.1ms preprocess, 1666.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1810.0ms\n",
      "Speed: 6.0ms preprocess, 1810.0ms inference, 4.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2434.6ms\n",
      "Speed: 14.7ms preprocess, 2434.6ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.2ms\n",
      "Speed: 6.4ms preprocess, 1663.2ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.1ms\n",
      "Speed: 9.1ms preprocess, 1810.1ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 480x640 1 person, 1903.2ms\n",
      "Speed: 9.6ms preprocess, 1903.2ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.9ms\n",
      "Speed: 6.2ms preprocess, 1810.9ms inference, 4.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2435.2ms\n",
      "Speed: 13.8ms preprocess, 2435.2ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.7ms\n",
      "Speed: 6.6ms preprocess, 1808.7ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.8ms\n",
      "Speed: 7.0ms preprocess, 1905.8ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.1ms\n",
      "Speed: 21.3ms preprocess, 1904.1ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 (no detections), 1809.7ms\n",
      "Speed: 5.0ms preprocess, 1809.7ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.9ms\n",
      "Speed: 10.3ms preprocess, 1982.9ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.1ms\n",
      "Speed: 5.0ms preprocess, 1665.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1902.4ms\n",
      "Speed: 11.6ms preprocess, 1902.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 1569.1ms\n",
      "Speed: 5.9ms preprocess, 1569.1ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 3 persons, 1567.2ms\n",
      "Speed: 8.5ms preprocess, 1567.2ms inference, 4.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 person, 1566.5ms\n",
      "Speed: 4.0ms preprocess, 1566.5ms inference, 5.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.1ms\n",
      "Speed: 7.0ms preprocess, 1812.1ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.0ms\n",
      "Speed: 8.0ms preprocess, 1904.0ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1902.4ms\n",
      "Speed: 9.0ms preprocess, 1902.4ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.3ms\n",
      "Speed: 9.1ms preprocess, 1982.3ms inference, 2.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 5.0ms preprocess, 1809.9ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.8ms\n",
      "Speed: 4.0ms preprocess, 1663.8ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.8ms\n",
      "Speed: 10.0ms preprocess, 1903.8ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1900.3ms\n",
      "Speed: 9.7ms preprocess, 1900.3ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1902.7ms\n",
      "Speed: 9.3ms preprocess, 1902.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1665.4ms\n",
      "Speed: 4.0ms preprocess, 1665.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.7ms\n",
      "Speed: 11.9ms preprocess, 1982.7ms inference, 6.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1661.1ms\n",
      "Speed: 6.2ms preprocess, 1661.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1902.7ms\n",
      "Speed: 10.0ms preprocess, 1902.7ms inference, 7.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1662.8ms\n",
      "Speed: 5.5ms preprocess, 1662.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.3ms\n",
      "Speed: 6.8ms preprocess, 1983.3ms inference, 5.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 544x640 1 person, 2231.5ms\n",
      "Speed: 6.9ms preprocess, 2231.5ms inference, 4.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.3ms\n",
      "Speed: 7.0ms preprocess, 1812.3ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1901.9ms\n",
      "Speed: 11.7ms preprocess, 1901.9ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.0ms\n",
      "Speed: 5.3ms preprocess, 1663.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.7ms\n",
      "Speed: 9.0ms preprocess, 1907.7ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1669.2ms\n",
      "Speed: 8.0ms preprocess, 1669.2ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.0ms\n",
      "Speed: 9.0ms preprocess, 1809.0ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1768.0ms\n",
      "Speed: 5.7ms preprocess, 1768.0ms inference, 6.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x480 1 person, 1809.0ms\n",
      "Speed: 8.4ms preprocess, 1809.0ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 1809.7ms\n",
      "Speed: 8.0ms preprocess, 1809.7ms inference, 4.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1905.7ms\n",
      "Speed: 5.4ms preprocess, 1905.7ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.4ms\n",
      "Speed: 6.4ms preprocess, 1809.4ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1772.2ms\n",
      "Speed: 5.0ms preprocess, 1772.2ms inference, 4.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.9ms\n",
      "Speed: 8.6ms preprocess, 1812.9ms inference, 3.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.7ms\n",
      "Speed: 4.3ms preprocess, 1665.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1905.6ms\n",
      "Speed: 5.0ms preprocess, 1905.6ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.8ms\n",
      "Speed: 7.9ms preprocess, 1903.8ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.0ms\n",
      "Speed: 8.5ms preprocess, 2439.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.3ms\n",
      "Speed: 8.0ms preprocess, 1663.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x416 1 person, 1729.2ms\n",
      "Speed: 6.0ms preprocess, 1729.2ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x448 1 person, 1763.2ms\n",
      "Speed: 10.5ms preprocess, 1763.2ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2439.1ms\n",
      "Speed: 6.7ms preprocess, 2439.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.8ms\n",
      "Speed: 6.0ms preprocess, 1808.8ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1809.3ms\n",
      "Speed: 10.0ms preprocess, 1809.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 2442.5ms\n",
      "Speed: 10.1ms preprocess, 2442.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.0ms\n",
      "Speed: 5.0ms preprocess, 1763.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2438.0ms\n",
      "Speed: 12.8ms preprocess, 2438.0ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1981.7ms\n",
      "Speed: 8.5ms preprocess, 1981.7ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x576 4 persons, 2329.1ms\n",
      "Speed: 6.2ms preprocess, 2329.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 1 person, 2436.1ms\n",
      "Speed: 10.9ms preprocess, 2436.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1760.4ms\n",
      "Speed: 18.6ms preprocess, 1760.4ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x544 1 person, 2285.1ms\n",
      "Speed: 6.4ms preprocess, 2285.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 2437.8ms\n",
      "Speed: 9.6ms preprocess, 2437.8ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.4ms\n",
      "Speed: 3.6ms preprocess, 1809.4ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.5ms\n",
      "Speed: 6.4ms preprocess, 2438.5ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1770.7ms\n",
      "Speed: 5.0ms preprocess, 1770.7ms inference, 3.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.9ms\n",
      "Speed: 6.0ms preprocess, 1667.9ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.4ms\n",
      "Speed: 5.3ms preprocess, 1813.4ms inference, 7.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1809.6ms\n",
      "Speed: 6.2ms preprocess, 1809.6ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1667.6ms\n",
      "Speed: 6.0ms preprocess, 1667.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 1760.4ms\n",
      "Speed: 5.0ms preprocess, 1760.4ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 512x640 1 person, 1982.9ms\n",
      "Speed: 6.4ms preprocess, 1982.9ms inference, 3.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1807.8ms\n",
      "Speed: 6.7ms preprocess, 1807.8ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.1ms\n",
      "Speed: 8.0ms preprocess, 2438.1ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x576 1 person, 2329.0ms\n",
      "Speed: 7.1ms preprocess, 2329.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x576 2 persons, 2327.3ms\n",
      "Speed: 12.0ms preprocess, 2327.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 1 person, 1670.8ms\n",
      "Speed: 6.1ms preprocess, 1670.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.1ms\n",
      "Speed: 8.9ms preprocess, 1810.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x608 1 person, 2356.7ms\n",
      "Speed: 7.2ms preprocess, 2356.7ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 1 person, 2436.7ms\n",
      "Speed: 10.0ms preprocess, 2436.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.7ms\n",
      "Speed: 6.0ms preprocess, 1810.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x512 1 person, 1876.0ms\n",
      "Speed: 9.4ms preprocess, 1876.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x448 1 person, 1759.5ms\n",
      "Speed: 6.0ms preprocess, 1759.5ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 416x640 1 person, 1769.4ms\n",
      "Speed: 10.1ms preprocess, 1769.4ms inference, 4.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x480 1 person, 1807.2ms\n",
      "Speed: 7.5ms preprocess, 1807.2ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x544 1 person, 2288.7ms\n",
      "Speed: 7.5ms preprocess, 2288.7ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x448 1 person, 1761.5ms\n",
      "Speed: 10.0ms preprocess, 1761.5ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1665.8ms\n",
      "Speed: 13.6ms preprocess, 1665.8ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.5ms\n",
      "Speed: 8.0ms preprocess, 1810.5ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 2438.8ms\n",
      "Speed: 8.5ms preprocess, 2438.8ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.4ms\n",
      "Speed: 5.2ms preprocess, 1665.4ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.3ms\n",
      "Speed: 8.4ms preprocess, 1663.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.7ms\n",
      "Speed: 6.4ms preprocess, 1763.7ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Error: Could not read image from D:\\archith\\DATASET\\TRAIN\\tree\\00000114.jpg\n",
      "\n",
      "0: 640x448 1 person, 1758.9ms\n",
      "Speed: 5.2ms preprocess, 1758.9ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1758.1ms\n",
      "Speed: 6.0ms preprocess, 1758.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x576 1 person, 2329.4ms\n",
      "Speed: 6.0ms preprocess, 2329.4ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 1 person, 1664.8ms\n",
      "Speed: 7.1ms preprocess, 1664.8ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 person, 1873.8ms\n",
      "Speed: 10.3ms preprocess, 1873.8ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x416 1 person, 1717.7ms\n",
      "Speed: 8.0ms preprocess, 1717.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 384x640 1 person, 1666.1ms\n",
      "Speed: 13.9ms preprocess, 1666.1ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1811.3ms\n",
      "Speed: 7.0ms preprocess, 1811.3ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.8ms\n",
      "Speed: 8.7ms preprocess, 1810.8ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 person, 1806.5ms\n",
      "Speed: 7.2ms preprocess, 1806.5ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x608 1 person, 2359.1ms\n",
      "Speed: 12.7ms preprocess, 2359.1ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 352x640 1 person, 1571.3ms\n",
      "Speed: 5.0ms preprocess, 1571.3ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.5ms\n",
      "Speed: 7.0ms preprocess, 1664.5ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.6ms\n",
      "Speed: 14.9ms preprocess, 2440.6ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1809.4ms\n",
      "Speed: 6.0ms preprocess, 1809.4ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x512 1 person, 1873.6ms\n",
      "Speed: 6.1ms preprocess, 1873.6ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 384x640 1 person, 1666.3ms\n",
      "Speed: 8.5ms preprocess, 1666.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 1770.4ms\n",
      "Speed: 5.2ms preprocess, 1770.4ms inference, 4.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x480 1 person, 1809.3ms\n",
      "Speed: 9.0ms preprocess, 1809.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x544 1 person, 2288.9ms\n",
      "Speed: 14.4ms preprocess, 2288.9ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x448 2 persons, 1760.1ms\n",
      "Speed: 9.0ms preprocess, 1760.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2436.2ms\n",
      "Speed: 7.8ms preprocess, 2436.2ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1759.6ms\n",
      "Speed: 6.7ms preprocess, 1759.6ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 352x640 1 person, 1570.6ms\n",
      "Speed: 5.6ms preprocess, 1570.6ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.3ms\n",
      "Speed: 7.2ms preprocess, 1808.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1771.3ms\n",
      "Speed: 9.4ms preprocess, 1771.3ms inference, 5.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x448 1 person, 1759.2ms\n",
      "Speed: 5.4ms preprocess, 1759.2ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1760.0ms\n",
      "Speed: 5.0ms preprocess, 1760.0ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1663.4ms\n",
      "Speed: 6.0ms preprocess, 1663.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 1760.8ms\n",
      "Speed: 6.7ms preprocess, 1760.8ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x384 1 person, 1670.3ms\n",
      "Speed: 6.6ms preprocess, 1670.3ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 448x640 1 person, 1807.5ms\n",
      "Speed: 6.5ms preprocess, 1807.5ms inference, 5.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.7ms\n",
      "Speed: 10.0ms preprocess, 1810.7ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1662.0ms\n",
      "Speed: 5.1ms preprocess, 1662.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x256 1 person, 1156.1ms\n",
      "Speed: 4.1ms preprocess, 1156.1ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x448 2 persons, 1761.6ms\n",
      "Speed: 6.1ms preprocess, 1761.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x544 1 person, 2286.5ms\n",
      "Speed: 8.0ms preprocess, 2286.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 512x640 1 person, 1982.7ms\n",
      "Speed: 13.3ms preprocess, 1982.7ms inference, 6.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.0ms\n",
      "Speed: 9.0ms preprocess, 1664.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 1570.3ms\n",
      "Speed: 7.0ms preprocess, 1570.3ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2435.6ms\n",
      "Speed: 8.5ms preprocess, 2435.6ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1807.6ms\n",
      "Speed: 6.0ms preprocess, 1807.6ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x384 1 person, 1670.6ms\n",
      "Speed: 5.0ms preprocess, 1670.6ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 352x640 1 person, 1569.9ms\n",
      "Speed: 10.0ms preprocess, 1569.9ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.1ms\n",
      "Speed: 5.0ms preprocess, 1664.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.6ms\n",
      "Speed: 8.5ms preprocess, 1982.6ms inference, 3.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x480 1 person, 1811.1ms\n",
      "Speed: 7.0ms preprocess, 1811.1ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x256 1 person, 1157.3ms\n",
      "Speed: 4.0ms preprocess, 1157.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x544 1 person, 2286.6ms\n",
      "Speed: 8.0ms preprocess, 2286.6ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x448 2 persons, 1762.0ms\n",
      "Speed: 6.1ms preprocess, 1762.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1666.1ms\n",
      "Speed: 4.7ms preprocess, 1666.1ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 1 person, 2288.5ms\n",
      "Speed: 8.0ms preprocess, 2288.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 416x640 1 person, 1769.4ms\n",
      "Speed: 12.2ms preprocess, 1769.4ms inference, 4.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.2ms\n",
      "Speed: 13.7ms preprocess, 1809.2ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1909.5ms\n",
      "Speed: 10.2ms preprocess, 1909.5ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 1811.4ms\n",
      "Speed: 13.1ms preprocess, 1811.4ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 2435.2ms\n",
      "Speed: 10.9ms preprocess, 2435.2ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.0ms\n",
      "Speed: 3.7ms preprocess, 1664.0ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.3ms\n",
      "Speed: 5.9ms preprocess, 1664.3ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1763.6ms\n",
      "Speed: 14.4ms preprocess, 1763.6ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x352 1 person, 1638.6ms\n",
      "Speed: 6.7ms preprocess, 1638.6ms inference, 8.9ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x448 1 person, 1767.9ms\n",
      "Speed: 12.2ms preprocess, 1767.9ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x480 1 person, 1815.9ms\n",
      "Speed: 12.4ms preprocess, 1815.9ms inference, 12.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x448 1 person, 1769.2ms\n",
      "Speed: 14.7ms preprocess, 1769.2ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2439.6ms\n",
      "Speed: 20.0ms preprocess, 2439.6ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.3ms\n",
      "Speed: 8.5ms preprocess, 1905.3ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x352 1 person, 1633.6ms\n",
      "Speed: 4.4ms preprocess, 1633.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x448 1 person, 1760.1ms\n",
      "Speed: 7.0ms preprocess, 1760.1ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1903.1ms\n",
      "Speed: 8.8ms preprocess, 1903.1ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 2 persons, 1808.9ms\n",
      "Speed: 6.4ms preprocess, 1808.9ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 2437.4ms\n",
      "Speed: 9.3ms preprocess, 2437.4ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1807.8ms\n",
      "Speed: 13.8ms preprocess, 1807.8ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1662.3ms\n",
      "Speed: 4.1ms preprocess, 1662.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 1575.1ms\n",
      "Speed: 4.0ms preprocess, 1575.1ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.9ms\n",
      "Speed: 5.5ms preprocess, 1662.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1762.2ms\n",
      "Speed: 5.8ms preprocess, 1762.2ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x384 1 person, 1670.3ms\n",
      "Speed: 4.2ms preprocess, 1670.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x448 1 person, 1761.8ms\n",
      "Speed: 6.0ms preprocess, 1761.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1756.1ms\n",
      "Speed: 4.5ms preprocess, 1756.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x480 1 person, 1810.6ms\n",
      "Speed: 5.8ms preprocess, 1810.6ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x448 1 person, 1763.8ms\n",
      "Speed: 5.5ms preprocess, 1763.8ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x480 1 person, 1806.7ms\n",
      "Speed: 7.2ms preprocess, 1806.7ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x352 1 person, 1634.0ms\n",
      "Speed: 4.9ms preprocess, 1634.0ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x480 1 person, 1808.6ms\n",
      "Speed: 5.7ms preprocess, 1808.6ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x384 1 person, 1672.4ms\n",
      "Speed: 7.2ms preprocess, 1672.4ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 448x640 1 person, 1811.1ms\n",
      "Speed: 8.4ms preprocess, 1811.1ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 1 person, 1719.2ms\n",
      "Speed: 6.6ms preprocess, 1719.2ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x416 1 person, 1717.1ms\n",
      "Speed: 6.3ms preprocess, 1717.1ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 416x640 1 person, 1772.0ms\n",
      "Speed: 6.5ms preprocess, 1772.0ms inference, 6.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x448 1 person, 1756.9ms\n",
      "Speed: 7.2ms preprocess, 1756.9ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1755.3ms\n",
      "Speed: 5.3ms preprocess, 1755.3ms inference, 12.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1810.3ms\n",
      "Speed: 10.4ms preprocess, 1810.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1760.7ms\n",
      "Speed: 7.0ms preprocess, 1760.7ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x352 1 person, 1636.1ms\n",
      "Speed: 4.4ms preprocess, 1636.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x448 1 person, 1760.0ms\n",
      "Speed: 6.2ms preprocess, 1760.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2438.3ms\n",
      "Speed: 5.8ms preprocess, 2438.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1808.8ms\n",
      "Speed: 5.0ms preprocess, 1808.8ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 1807.8ms\n",
      "Speed: 6.0ms preprocess, 1807.8ms inference, 6.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.1ms\n",
      "Speed: 6.8ms preprocess, 1905.1ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2443.0ms\n",
      "Speed: 8.0ms preprocess, 2443.0ms inference, 12.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 2 persons, 1572.7ms\n",
      "Speed: 12.0ms preprocess, 1572.7ms inference, 6.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x448 1 person, 1762.2ms\n",
      "Speed: 7.2ms preprocess, 1762.2ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1758.9ms\n",
      "Speed: 7.8ms preprocess, 1758.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x576 1 person, 2328.7ms\n",
      "Speed: 13.5ms preprocess, 2328.7ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 448x640 1 person, 1810.2ms\n",
      "Speed: 6.0ms preprocess, 1810.2ms inference, 6.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1761.7ms\n",
      "Speed: 6.1ms preprocess, 1761.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x416 1 person, 1720.4ms\n",
      "Speed: 5.6ms preprocess, 1720.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x512 1 person, 1870.1ms\n",
      "Speed: 5.6ms preprocess, 1870.1ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x448 1 person, 1759.0ms\n",
      "Speed: 6.0ms preprocess, 1759.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1903.8ms\n",
      "Speed: 6.3ms preprocess, 1903.8ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x544 1 person, 2289.8ms\n",
      "Speed: 7.5ms preprocess, 2289.8ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x448 1 person, 1763.2ms\n",
      "Speed: 5.5ms preprocess, 1763.2ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1665.9ms\n",
      "Speed: 3.6ms preprocess, 1665.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1660.0ms\n",
      "Speed: 5.4ms preprocess, 1660.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x640 1 person, 1096.1ms\n",
      "Speed: 4.4ms preprocess, 1096.1ms inference, 4.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 384x640 1 person, 1659.6ms\n",
      "Speed: 6.6ms preprocess, 1659.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.7ms\n",
      "Speed: 6.1ms preprocess, 1905.7ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.2ms\n",
      "Speed: 7.5ms preprocess, 1808.2ms inference, 4.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.5ms\n",
      "Speed: 7.1ms preprocess, 2439.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.0ms\n",
      "Speed: 7.6ms preprocess, 2436.0ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.0ms\n",
      "Speed: 8.7ms preprocess, 2438.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1809.4ms\n",
      "Speed: 7.7ms preprocess, 1809.4ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 512x640 1 person, 1982.9ms\n",
      "Speed: 5.0ms preprocess, 1982.9ms inference, 6.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2434.5ms\n",
      "Speed: 6.3ms preprocess, 2434.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 person, 1720.0ms\n",
      "Speed: 5.2ms preprocess, 1720.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 person, 2436.8ms\n",
      "Speed: 6.6ms preprocess, 2436.8ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1570.7ms\n",
      "Speed: 5.3ms preprocess, 1570.7ms inference, 7.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.7ms\n",
      "Speed: 6.0ms preprocess, 1809.7ms inference, 6.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.7ms\n",
      "Speed: 8.7ms preprocess, 2437.7ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.6ms\n",
      "Speed: 5.6ms preprocess, 1807.6ms inference, 5.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.7ms\n",
      "Speed: 5.3ms preprocess, 1807.7ms inference, 4.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.5ms\n",
      "Speed: 7.5ms preprocess, 2438.5ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.8ms\n",
      "Speed: 6.0ms preprocess, 1808.8ms inference, 5.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.1ms\n",
      "Speed: 7.6ms preprocess, 2436.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1663.1ms\n",
      "Speed: 4.4ms preprocess, 1663.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1986.1ms\n",
      "Speed: 6.3ms preprocess, 1986.1ms inference, 3.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 352x640 1 person, 1572.8ms\n",
      "Speed: 4.9ms preprocess, 1572.8ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.0ms\n",
      "Speed: 4.7ms preprocess, 1809.0ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 1 person, 2231.5ms\n",
      "Speed: 6.0ms preprocess, 2231.5ms inference, 3.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x512 1 person, 1876.3ms\n",
      "Speed: 6.0ms preprocess, 1876.3ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x480 1 person, 1805.5ms\n",
      "Speed: 12.0ms preprocess, 1805.5ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 544x640 1 person, 2228.2ms\n",
      "Speed: 8.8ms preprocess, 2228.2ms inference, 6.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.0ms\n",
      "Speed: 6.6ms preprocess, 1665.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.3ms\n",
      "Speed: 6.5ms preprocess, 1904.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1902.4ms\n",
      "Speed: 7.0ms preprocess, 1902.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.1ms\n",
      "Speed: 7.9ms preprocess, 2439.1ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 person, 1876.1ms\n",
      "Speed: 8.3ms preprocess, 1876.1ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x480 1 person, 1810.0ms\n",
      "Speed: 6.6ms preprocess, 1810.0ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 480x640 1 person, 1902.1ms\n",
      "Speed: 6.7ms preprocess, 1902.1ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.3ms\n",
      "Speed: 5.0ms preprocess, 1665.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.0ms\n",
      "Speed: 4.5ms preprocess, 1662.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.6ms\n",
      "Speed: 5.2ms preprocess, 1903.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2232.1ms\n",
      "Speed: 6.0ms preprocess, 2232.1ms inference, 5.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.0ms\n",
      "Speed: 5.5ms preprocess, 1663.0ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.3ms\n",
      "Speed: 5.7ms preprocess, 1810.3ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1664.4ms\n",
      "Speed: 6.0ms preprocess, 1664.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.9ms\n",
      "Speed: 7.3ms preprocess, 1903.9ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.9ms\n",
      "Speed: 5.9ms preprocess, 2229.9ms inference, 5.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.1ms\n",
      "Speed: 7.4ms preprocess, 1665.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.5ms\n",
      "Speed: 7.4ms preprocess, 1907.5ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.5ms\n",
      "Speed: 6.5ms preprocess, 1763.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1909.4ms\n",
      "Speed: 5.0ms preprocess, 1909.4ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.9ms\n",
      "Speed: 5.7ms preprocess, 2436.9ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1905.8ms\n",
      "Speed: 6.9ms preprocess, 1905.8ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1768.5ms\n",
      "Speed: 7.0ms preprocess, 1768.5ms inference, 5.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.6ms\n",
      "Speed: 6.2ms preprocess, 1907.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.2ms\n",
      "Speed: 5.5ms preprocess, 1808.2ms inference, 6.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.0ms\n",
      "Speed: 6.0ms preprocess, 1808.0ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.4ms\n",
      "Speed: 4.4ms preprocess, 1807.4ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.7ms\n",
      "Speed: 7.2ms preprocess, 2436.7ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.0ms\n",
      "Speed: 6.4ms preprocess, 1662.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 1770.5ms\n",
      "Speed: 4.9ms preprocess, 1770.5ms inference, 3.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.5ms\n",
      "Speed: 5.8ms preprocess, 1810.5ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1668.1ms\n",
      "Speed: 4.2ms preprocess, 1668.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1907.2ms\n",
      "Speed: 5.3ms preprocess, 1907.2ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.3ms\n",
      "Speed: 9.5ms preprocess, 1903.3ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.7ms\n",
      "Speed: 6.3ms preprocess, 1810.7ms inference, 13.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.6ms\n",
      "Speed: 5.6ms preprocess, 1808.6ms inference, 6.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1771.1ms\n",
      "Speed: 5.1ms preprocess, 1771.1ms inference, 6.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.3ms\n",
      "Speed: 5.3ms preprocess, 1664.3ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.8ms\n",
      "Speed: 9.5ms preprocess, 2437.8ms inference, 10.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1770.9ms\n",
      "Speed: 4.3ms preprocess, 1770.9ms inference, 6.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.7ms\n",
      "Speed: 8.3ms preprocess, 1810.7ms inference, 10.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.7ms\n",
      "Speed: 5.0ms preprocess, 1665.7ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 1 person, 1808.5ms\n",
      "Speed: 6.3ms preprocess, 1808.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 2436.3ms\n",
      "Speed: 8.4ms preprocess, 2436.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.5ms\n",
      "Speed: 5.0ms preprocess, 1982.5ms inference, 4.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1807.1ms\n",
      "Speed: 6.2ms preprocess, 1807.1ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.7ms\n",
      "Speed: 5.4ms preprocess, 1807.7ms inference, 5.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.0ms\n",
      "Speed: 5.5ms preprocess, 1807.0ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1660.9ms\n",
      "Speed: 5.2ms preprocess, 1660.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.4ms\n",
      "Speed: 7.2ms preprocess, 1812.4ms inference, 4.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 1571.1ms\n",
      "Speed: 4.3ms preprocess, 1571.1ms inference, 4.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.8ms\n",
      "Speed: 16.8ms preprocess, 1982.8ms inference, 3.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.7ms\n",
      "Speed: 6.1ms preprocess, 1904.7ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x576 1 person, 2326.4ms\n",
      "Speed: 7.3ms preprocess, 2326.4ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 5 persons, 2437.3ms\n",
      "Speed: 7.6ms preprocess, 2437.3ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.9ms\n",
      "Speed: 6.1ms preprocess, 1983.9ms inference, 4.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.2ms\n",
      "Speed: 6.2ms preprocess, 1982.2ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1763.2ms\n",
      "Speed: 8.3ms preprocess, 1763.2ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 320x640 1 person, 1493.1ms\n",
      "Speed: 3.5ms preprocess, 1493.1ms inference, 7.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 5.5ms preprocess, 1809.9ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.8ms\n",
      "Speed: 7.3ms preprocess, 2438.8ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2231.9ms\n",
      "Speed: 5.9ms preprocess, 2231.9ms inference, 4.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.9ms\n",
      "Speed: 4.6ms preprocess, 1908.9ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.7ms\n",
      "Speed: 9.0ms preprocess, 2437.7ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.0ms\n",
      "Speed: 7.4ms preprocess, 2229.0ms inference, 8.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.3ms\n",
      "Speed: 7.0ms preprocess, 1809.3ms inference, 6.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.5ms\n",
      "Speed: 8.6ms preprocess, 1808.5ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.5ms\n",
      "Speed: 8.4ms preprocess, 1665.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1659.9ms\n",
      "Speed: 5.3ms preprocess, 1659.9ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.0ms\n",
      "Speed: 10.3ms preprocess, 2438.0ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.4ms\n",
      "Speed: 6.0ms preprocess, 1908.4ms inference, 7.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2230.4ms\n",
      "Speed: 9.3ms preprocess, 2230.4ms inference, 4.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.3ms\n",
      "Speed: 3.8ms preprocess, 1811.3ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.2ms\n",
      "Speed: 16.8ms preprocess, 1667.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.5ms\n",
      "Speed: 5.8ms preprocess, 1809.5ms inference, 5.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.2ms\n",
      "Speed: 4.5ms preprocess, 1807.2ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.6ms\n",
      "Speed: 5.8ms preprocess, 1807.6ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.8ms\n",
      "Speed: 6.6ms preprocess, 2437.8ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 person, 1875.3ms\n",
      "Speed: 4.4ms preprocess, 1875.3ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 384x640 1 person, 1663.3ms\n",
      "Speed: 3.5ms preprocess, 1663.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.0ms\n",
      "Speed: 7.1ms preprocess, 1809.0ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.2ms\n",
      "Speed: 8.4ms preprocess, 2439.2ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1665.3ms\n",
      "Speed: 7.1ms preprocess, 1665.3ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 person, 1871.5ms\n",
      "Speed: 8.2ms preprocess, 1871.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x448 1 person, 1759.5ms\n",
      "Speed: 5.5ms preprocess, 1759.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1663.2ms\n",
      "Speed: 7.8ms preprocess, 1663.2ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.1ms\n",
      "Speed: 4.2ms preprocess, 1662.1ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 320x640 2 persons, 1496.1ms\n",
      "Speed: 3.4ms preprocess, 1496.1ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x512 1 person, 1870.9ms\n",
      "Speed: 6.6ms preprocess, 1870.9ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 2 persons, 2434.3ms\n",
      "Speed: 9.1ms preprocess, 2434.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.6ms\n",
      "Speed: 4.4ms preprocess, 1807.6ms inference, 8.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.3ms\n",
      "Speed: 4.0ms preprocess, 1666.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 1 person, 1811.3ms\n",
      "Speed: 5.2ms preprocess, 1811.3ms inference, 7.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 576x640 1 person, 2278.5ms\n",
      "Speed: 9.4ms preprocess, 2278.5ms inference, 5.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x608 1 person, 2357.3ms\n",
      "Speed: 6.4ms preprocess, 2357.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x416 1 person, 1719.0ms\n",
      "Speed: 4.2ms preprocess, 1719.0ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x544 1 person, 2286.1ms\n",
      "Speed: 5.4ms preprocess, 2286.1ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 2436.2ms\n",
      "Speed: 7.4ms preprocess, 2436.2ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2434.2ms\n",
      "Speed: 8.4ms preprocess, 2434.2ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.3ms\n",
      "Speed: 4.7ms preprocess, 1663.3ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1906.1ms\n",
      "Speed: 6.1ms preprocess, 1906.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.2ms\n",
      "Speed: 6.2ms preprocess, 1663.2ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.9ms\n",
      "Speed: 5.1ms preprocess, 1664.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.5ms\n",
      "Speed: 6.3ms preprocess, 1662.5ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.6ms\n",
      "Speed: 7.8ms preprocess, 2439.6ms inference, 8.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 256x640 1 person, 1172.5ms\n",
      "Speed: 4.8ms preprocess, 1172.5ms inference, 9.0ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 640x576 3 persons, 2329.4ms\n",
      "Speed: 10.5ms preprocess, 2329.4ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 1 person, 2437.2ms\n",
      "Speed: 8.5ms preprocess, 2437.2ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2232.3ms\n",
      "Speed: 8.3ms preprocess, 2232.3ms inference, 4.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x512 1 person, 1870.7ms\n",
      "Speed: 5.7ms preprocess, 1870.7ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 384x640 1 person, 1664.5ms\n",
      "Speed: 7.7ms preprocess, 1664.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.8ms\n",
      "Speed: 8.8ms preprocess, 1661.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 2280.5ms\n",
      "Speed: 7.6ms preprocess, 2280.5ms inference, 5.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.3ms\n",
      "Speed: 7.2ms preprocess, 1663.3ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.1ms\n",
      "Speed: 4.0ms preprocess, 1665.1ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 1568.5ms\n",
      "Speed: 4.3ms preprocess, 1568.5ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.6ms\n",
      "Speed: 5.6ms preprocess, 1811.6ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.5ms\n",
      "Speed: 6.4ms preprocess, 1808.5ms inference, 4.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 2288.6ms\n",
      "Speed: 6.9ms preprocess, 2288.6ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 1 person, 1814.8ms\n",
      "Speed: 5.1ms preprocess, 1814.8ms inference, 7.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.0ms\n",
      "Speed: 5.2ms preprocess, 1907.0ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1663.0ms\n",
      "Speed: 4.4ms preprocess, 1663.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2434.0ms\n",
      "Speed: 8.2ms preprocess, 2434.0ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.1ms\n",
      "Speed: 5.1ms preprocess, 1904.1ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 1807.4ms\n",
      "Speed: 6.0ms preprocess, 1807.4ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1663.6ms\n",
      "Speed: 5.1ms preprocess, 1663.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.0ms\n",
      "Speed: 4.8ms preprocess, 1810.0ms inference, 3.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.0ms\n",
      "Speed: 5.6ms preprocess, 1664.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.5ms\n",
      "Speed: 6.8ms preprocess, 1983.5ms inference, 7.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.5ms\n",
      "Speed: 5.7ms preprocess, 1808.5ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 256x640 1 person, 1163.2ms\n",
      "Speed: 3.9ms preprocess, 1163.2ms inference, 7.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.7ms\n",
      "Speed: 8.3ms preprocess, 1811.7ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1808.0ms\n",
      "Speed: 6.4ms preprocess, 1808.0ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1661.1ms\n",
      "Speed: 4.2ms preprocess, 1661.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 2281.1ms\n",
      "Speed: 9.7ms preprocess, 2281.1ms inference, 5.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.9ms\n",
      "Speed: 6.7ms preprocess, 1904.9ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 4.0ms preprocess, 1809.9ms inference, 3.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 11 persons, 2231.1ms\n",
      "Speed: 6.2ms preprocess, 2231.1ms inference, 94.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.9ms\n",
      "Speed: 6.8ms preprocess, 1810.9ms inference, 5.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1805.2ms\n",
      "Speed: 4.0ms preprocess, 1805.2ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.6ms\n",
      "Speed: 5.6ms preprocess, 1661.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.0ms\n",
      "Speed: 7.0ms preprocess, 2229.0ms inference, 7.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.8ms\n",
      "Speed: 4.9ms preprocess, 1664.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.4ms\n",
      "Speed: 4.8ms preprocess, 1810.4ms inference, 4.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.2ms\n",
      "Speed: 11.0ms preprocess, 1905.2ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.9ms\n",
      "Speed: 5.6ms preprocess, 1984.9ms inference, 4.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.6ms\n",
      "Speed: 5.5ms preprocess, 1663.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1662.5ms\n",
      "Speed: 5.6ms preprocess, 1662.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 2279.3ms\n",
      "Speed: 7.0ms preprocess, 2279.3ms inference, 4.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.4ms\n",
      "Speed: 7.7ms preprocess, 1906.4ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.9ms\n",
      "Speed: 4.8ms preprocess, 1665.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.8ms\n",
      "Speed: 4.2ms preprocess, 1661.8ms inference, 9.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.8ms\n",
      "Speed: 7.5ms preprocess, 2436.8ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1908.6ms\n",
      "Speed: 5.5ms preprocess, 1908.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x608 1 person, 2357.3ms\n",
      "Speed: 8.6ms preprocess, 2357.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 1 person, 1664.8ms\n",
      "Speed: 5.0ms preprocess, 1664.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.8ms\n",
      "Speed: 6.2ms preprocess, 1983.8ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 576x640 1 person, 2278.9ms\n",
      "Speed: 6.6ms preprocess, 2278.9ms inference, 7.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.6ms\n",
      "Speed: 5.6ms preprocess, 1663.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1810.6ms\n",
      "Speed: 7.4ms preprocess, 1810.6ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1770.6ms\n",
      "Speed: 5.4ms preprocess, 1770.6ms inference, 7.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1662.8ms\n",
      "Speed: 4.7ms preprocess, 1662.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.2ms\n",
      "Speed: 8.4ms preprocess, 1904.2ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.0ms\n",
      "Speed: 6.6ms preprocess, 2436.0ms inference, 7.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.2ms\n",
      "Speed: 5.5ms preprocess, 1812.2ms inference, 11.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.9ms\n",
      "Speed: 7.5ms preprocess, 2438.9ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.2ms\n",
      "Speed: 5.1ms preprocess, 1666.2ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.9ms\n",
      "Speed: 7.5ms preprocess, 2436.9ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 person, 1674.8ms\n",
      "Speed: 7.0ms preprocess, 1674.8ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x640 1 person, 2437.3ms\n",
      "Speed: 10.2ms preprocess, 2437.3ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.1ms\n",
      "Speed: 5.6ms preprocess, 1904.1ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.2ms\n",
      "Speed: 6.3ms preprocess, 2229.2ms inference, 5.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.8ms\n",
      "Speed: 5.0ms preprocess, 1663.8ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 7.4ms preprocess, 1809.9ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 1570.0ms\n",
      "Speed: 5.8ms preprocess, 1570.0ms inference, 4.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 416x640 1 person, 1771.9ms\n",
      "Speed: 6.0ms preprocess, 1771.9ms inference, 9.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.8ms\n",
      "Speed: 7.2ms preprocess, 2440.8ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.0ms\n",
      "Speed: 7.8ms preprocess, 1809.0ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1760.4ms\n",
      "Speed: 4.6ms preprocess, 1760.4ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x480 1 person, 1806.7ms\n",
      "Speed: 6.3ms preprocess, 1806.7ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 2 persons, 2435.9ms\n",
      "Speed: 6.8ms preprocess, 2435.9ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.9ms\n",
      "Speed: 5.2ms preprocess, 1662.9ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.1ms\n",
      "Speed: 5.5ms preprocess, 1663.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.6ms\n",
      "Speed: 6.6ms preprocess, 1905.6ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.1ms\n",
      "Speed: 8.7ms preprocess, 2439.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.1ms\n",
      "Speed: 5.8ms preprocess, 1665.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.9ms\n",
      "Speed: 6.3ms preprocess, 1984.9ms inference, 6.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2442.0ms\n",
      "Speed: 7.2ms preprocess, 2442.0ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.8ms\n",
      "Speed: 5.3ms preprocess, 1663.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x608 1 person, 2360.2ms\n",
      "Speed: 9.7ms preprocess, 2360.2ms inference, 7.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 480x640 1 person, 1908.2ms\n",
      "Speed: 6.3ms preprocess, 1908.2ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.7ms\n",
      "Speed: 6.0ms preprocess, 1809.7ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.3ms\n",
      "Speed: 7.9ms preprocess, 2439.3ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1662.7ms\n",
      "Speed: 3.7ms preprocess, 1662.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x608 1 person, 2356.4ms\n",
      "Speed: 8.7ms preprocess, 2356.4ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 1 person, 2439.3ms\n",
      "Speed: 11.5ms preprocess, 2439.3ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.4ms\n",
      "Speed: 7.3ms preprocess, 1907.4ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.5ms\n",
      "Speed: 6.6ms preprocess, 1810.5ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 2290.5ms\n",
      "Speed: 6.5ms preprocess, 2290.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x576 1 person, 2331.8ms\n",
      "Speed: 6.2ms preprocess, 2331.8ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 480x640 1 person, 1905.6ms\n",
      "Speed: 9.5ms preprocess, 1905.6ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1771.3ms\n",
      "Speed: 8.0ms preprocess, 1771.3ms inference, 8.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.1ms\n",
      "Speed: 8.8ms preprocess, 1907.1ms inference, 8.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 576x640 1 person, 2282.9ms\n",
      "Speed: 12.1ms preprocess, 2282.9ms inference, 6.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.6ms\n",
      "Speed: 7.0ms preprocess, 2437.6ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.0ms\n",
      "Speed: 11.9ms preprocess, 1907.0ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2230.6ms\n",
      "Speed: 7.4ms preprocess, 2230.6ms inference, 5.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.4ms\n",
      "Speed: 6.3ms preprocess, 1810.4ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.8ms\n",
      "Speed: 10.9ms preprocess, 1663.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.3ms\n",
      "Speed: 4.7ms preprocess, 1808.3ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 1 person, 2279.3ms\n",
      "Speed: 9.1ms preprocess, 2279.3ms inference, 5.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.2ms\n",
      "Speed: 4.9ms preprocess, 1664.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.0ms\n",
      "Speed: 6.0ms preprocess, 1906.0ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1804.9ms\n",
      "Speed: 5.9ms preprocess, 1804.9ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1668.4ms\n",
      "Speed: 4.5ms preprocess, 1668.4ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.9ms\n",
      "Speed: 4.8ms preprocess, 1661.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.3ms\n",
      "Speed: 6.4ms preprocess, 1809.3ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.9ms\n",
      "Speed: 5.0ms preprocess, 1663.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.1ms\n",
      "Speed: 5.6ms preprocess, 1903.1ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 2 persons, 1810.0ms\n",
      "Speed: 13.2ms preprocess, 1810.0ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 480x640 1 person, 1907.9ms\n",
      "Speed: 8.4ms preprocess, 1907.9ms inference, 10.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 608x640 1 person, 2352.9ms\n",
      "Speed: 12.0ms preprocess, 2352.9ms inference, 8.7ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    }
   ],
   "source": [
    "train_data = process_dataset(r\"D:\\archith\\DATASET\\TRAIN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ae47075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:27:25.907429Z",
     "iopub.status.busy": "2024-07-12T13:27:25.906903Z",
     "iopub.status.idle": "2024-07-12T13:27:26.877851Z",
     "shell.execute_reply": "2024-07-12T13:27:26.876992Z"
    },
    "papermill": {
     "duration": 1.382595,
     "end_time": "2024-07-12T13:27:26.926456",
     "exception": false,
     "start_time": "2024-07-12T13:27:25.543861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "34\n",
      "48\n",
      "50\n",
      "71\n",
      "86\n",
      "117\n",
      "118\n",
      "130\n",
      "145\n",
      "171\n",
      "181\n",
      "185\n",
      "197\n",
      "328\n",
      "335\n",
      "344\n",
      "517\n",
      "618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1048"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_3d = []\n",
    "count = 0\n",
    "for i in train_data:\n",
    "    if(len(i[0][0]) == 17):\n",
    "        train_data_3d.append(i[0][0]) \n",
    "    else: print(count)\n",
    "    count+=1\n",
    "trained_data = np.array(train_data_3d)\n",
    "len(trained_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0fd4e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 318.3ms\n",
      "Speed: 247.9ms preprocess, 318.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 211.7ms\n",
      "Speed: 2.4ms preprocess, 211.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 245.8ms\n",
      "Speed: 3.4ms preprocess, 245.8ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 244.6ms\n",
      "Speed: 4.4ms preprocess, 244.6ms inference, 2.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 215.3ms\n",
      "Speed: 2.1ms preprocess, 215.3ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 454.4ms\n",
      "Speed: 2.0ms preprocess, 454.4ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 258.3ms\n",
      "Speed: 3.3ms preprocess, 258.3ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 232.1ms\n",
      "Speed: 2.1ms preprocess, 232.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 326.3ms\n",
      "Speed: 5.4ms preprocess, 326.3ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 269.8ms\n",
      "Speed: 2.0ms preprocess, 269.8ms inference, 4.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 238.3ms\n",
      "Speed: 2.5ms preprocess, 238.3ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 230.4ms\n",
      "Speed: 4.5ms preprocess, 230.4ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 597.5ms\n",
      "Speed: 4.5ms preprocess, 597.5ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 257.4ms\n",
      "Speed: 6.0ms preprocess, 257.4ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 236.3ms\n",
      "Speed: 4.5ms preprocess, 236.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 667.9ms\n",
      "Speed: 3.7ms preprocess, 667.9ms inference, 2.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 268.1ms\n",
      "Speed: 3.2ms preprocess, 268.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 241.1ms\n",
      "Speed: 4.1ms preprocess, 241.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 493.8ms\n",
      "Speed: 4.1ms preprocess, 493.8ms inference, 4.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 576x640 1 person, 341.5ms\n",
      "Speed: 5.1ms preprocess, 341.5ms inference, 6.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 448x640 1 person, 568.4ms\n",
      "Speed: 3.1ms preprocess, 568.4ms inference, 52.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 287.5ms\n",
      "Speed: 7.0ms preprocess, 287.5ms inference, 6.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 699.4ms\n",
      "Speed: 5.2ms preprocess, 699.4ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 450.1ms\n",
      "Speed: 4.0ms preprocess, 450.1ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 701.3ms\n",
      "Speed: 2.3ms preprocess, 701.3ms inference, 4.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 607.8ms\n",
      "Speed: 4.4ms preprocess, 607.8ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 1 person, 598.2ms\n",
      "Speed: 3.0ms preprocess, 598.2ms inference, 4.0ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 448x640 1 person, 317.0ms\n",
      "Speed: 11.0ms preprocess, 317.0ms inference, 3.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1105.2ms\n",
      "Speed: 6.1ms preprocess, 1105.2ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 891.2ms\n",
      "Speed: 4.8ms preprocess, 891.2ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 657.3ms\n",
      "Speed: 4.2ms preprocess, 657.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 843.2ms\n",
      "Speed: 2.0ms preprocess, 843.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 839.4ms\n",
      "Speed: 3.8ms preprocess, 839.4ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1036.0ms\n",
      "Speed: 5.0ms preprocess, 1036.0ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1769.8ms\n",
      "Speed: 6.2ms preprocess, 1769.8ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.1ms\n",
      "Speed: 3.1ms preprocess, 1666.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.9ms\n",
      "Speed: 4.6ms preprocess, 1811.9ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.3ms\n",
      "Speed: 4.0ms preprocess, 1665.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.8ms\n",
      "Speed: 4.6ms preprocess, 1809.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.0ms\n",
      "Speed: 7.0ms preprocess, 1809.0ms inference, 4.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 1572.1ms\n",
      "Speed: 4.5ms preprocess, 1572.1ms inference, 14.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 288x640 1 person, 1438.9ms\n",
      "Speed: 5.5ms preprocess, 1438.9ms inference, 4.5ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 480x640 1 person, 1909.8ms\n",
      "Speed: 5.5ms preprocess, 1909.8ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.5ms\n",
      "Speed: 5.6ms preprocess, 1666.5ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 1570.1ms\n",
      "Speed: 3.8ms preprocess, 1570.1ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.3ms\n",
      "Speed: 5.4ms preprocess, 1811.3ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 1574.5ms\n",
      "Speed: 3.7ms preprocess, 1574.5ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.3ms\n",
      "Speed: 5.5ms preprocess, 1665.3ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.1ms\n",
      "Speed: 6.2ms preprocess, 1810.1ms inference, 5.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.5ms\n",
      "Speed: 6.8ms preprocess, 1905.5ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.7ms\n",
      "Speed: 5.8ms preprocess, 1665.7ms inference, 10.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1885.4ms\n",
      "Speed: 10.1ms preprocess, 1885.4ms inference, 116.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 2292.9ms\n",
      "Speed: 19.2ms preprocess, 2292.9ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 1 person, 1668.2ms\n",
      "Speed: 9.0ms preprocess, 1668.2ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 1572.2ms\n",
      "Speed: 6.7ms preprocess, 1572.2ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 576x640 1 person, 2281.3ms\n",
      "Speed: 8.5ms preprocess, 2281.3ms inference, 6.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.8ms\n",
      "Speed: 9.4ms preprocess, 2436.8ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1573.2ms\n",
      "Speed: 8.5ms preprocess, 1573.2ms inference, 8.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.4ms\n",
      "Speed: 8.1ms preprocess, 1812.4ms inference, 7.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.8ms\n",
      "Speed: 7.6ms preprocess, 1808.8ms inference, 6.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.1ms\n",
      "Speed: 8.6ms preprocess, 1807.1ms inference, 8.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.4ms\n",
      "Speed: 12.6ms preprocess, 2436.4ms inference, 17.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.7ms\n",
      "Speed: 7.0ms preprocess, 2437.7ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.9ms\n",
      "Speed: 13.4ms preprocess, 2438.9ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.5ms\n",
      "Speed: 11.1ms preprocess, 2436.5ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1572.1ms\n",
      "Speed: 7.7ms preprocess, 1572.1ms inference, 4.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.6ms\n",
      "Speed: 6.5ms preprocess, 1809.6ms inference, 7.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1772.7ms\n",
      "Speed: 6.6ms preprocess, 1772.7ms inference, 12.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 1669.9ms\n",
      "Speed: 6.9ms preprocess, 1669.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 1 person, 1572.3ms\n",
      "Speed: 7.7ms preprocess, 1572.3ms inference, 6.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 288x640 1 person, 1437.1ms\n",
      "Speed: 6.2ms preprocess, 1437.1ms inference, 7.6ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 640x512 1 person, 1879.6ms\n",
      "Speed: 8.6ms preprocess, 1879.6ms inference, 13.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x448 1 person, 1761.8ms\n",
      "Speed: 9.5ms preprocess, 1761.8ms inference, 11.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1814.9ms\n",
      "Speed: 14.6ms preprocess, 1814.9ms inference, 7.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2459.0ms\n",
      "Speed: 9.3ms preprocess, 2459.0ms inference, 18.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1823.8ms\n",
      "Speed: 13.9ms preprocess, 1823.8ms inference, 15.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1670.8ms\n",
      "Speed: 11.6ms preprocess, 1670.8ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.3ms\n",
      "Speed: 6.2ms preprocess, 1667.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.9ms\n",
      "Speed: 8.6ms preprocess, 1763.9ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2436.2ms\n",
      "Speed: 8.5ms preprocess, 2436.2ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.3ms\n",
      "Speed: 6.7ms preprocess, 1811.3ms inference, 5.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 1 person, 2232.6ms\n",
      "Speed: 6.8ms preprocess, 2232.6ms inference, 3.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.0ms\n",
      "Speed: 10.6ms preprocess, 2437.0ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.1ms\n",
      "Speed: 8.7ms preprocess, 2437.1ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.5ms\n",
      "Speed: 5.0ms preprocess, 1906.5ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.6ms\n",
      "Speed: 6.9ms preprocess, 2438.6ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1810.4ms\n",
      "Speed: 5.0ms preprocess, 1810.4ms inference, 5.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1909.2ms\n",
      "Speed: 9.1ms preprocess, 1909.2ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2441.2ms\n",
      "Speed: 6.3ms preprocess, 2441.2ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.4ms\n",
      "Speed: 4.4ms preprocess, 1666.4ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.8ms\n",
      "Speed: 7.3ms preprocess, 1810.8ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2440.6ms\n",
      "Speed: 7.1ms preprocess, 2440.6ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1814.3ms\n",
      "Speed: 5.8ms preprocess, 1814.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 1 person, 2233.1ms\n",
      "Speed: 7.4ms preprocess, 2233.1ms inference, 5.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.2ms\n",
      "Speed: 5.5ms preprocess, 1667.2ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.0ms\n",
      "Speed: 5.0ms preprocess, 1813.0ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.5ms\n",
      "Speed: 7.6ms preprocess, 2438.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.7ms\n",
      "Speed: 7.2ms preprocess, 2438.7ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.2ms\n",
      "Speed: 4.5ms preprocess, 1666.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.4ms\n",
      "Speed: 5.8ms preprocess, 1811.4ms inference, 6.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1688.6ms\n",
      "Speed: 4.8ms preprocess, 1688.6ms inference, 20.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 320x640 1 person, 1496.9ms\n",
      "Speed: 5.5ms preprocess, 1496.9ms inference, 5.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1812.8ms\n",
      "Speed: 16.3ms preprocess, 1812.8ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.8ms\n",
      "Speed: 5.9ms preprocess, 1810.8ms inference, 5.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1764.7ms\n",
      "Speed: 8.1ms preprocess, 1764.7ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1908.1ms\n",
      "Speed: 7.3ms preprocess, 1908.1ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2233.6ms\n",
      "Speed: 7.4ms preprocess, 2233.6ms inference, 7.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.8ms\n",
      "Speed: 7.1ms preprocess, 2440.8ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1668.9ms\n",
      "Speed: 7.6ms preprocess, 1668.9ms inference, 10.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1814.4ms\n",
      "Speed: 14.1ms preprocess, 1814.4ms inference, 10.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1811.4ms\n",
      "Speed: 9.3ms preprocess, 1811.4ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 person, 2437.7ms\n",
      "Speed: 6.7ms preprocess, 2437.7ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 1 person, 1445.7ms\n",
      "Speed: 6.0ms preprocess, 1445.7ms inference, 7.2ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.8ms\n",
      "Speed: 5.6ms preprocess, 1904.8ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.0ms\n",
      "Speed: 7.5ms preprocess, 2438.0ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.2ms\n",
      "Speed: 5.6ms preprocess, 1812.2ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1772.5ms\n",
      "Speed: 5.0ms preprocess, 1772.5ms inference, 4.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x480 1 person, 1812.9ms\n",
      "Speed: 15.9ms preprocess, 1812.9ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x448 3 persons, 1762.7ms\n",
      "Speed: 5.9ms preprocess, 1762.7ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1904.9ms\n",
      "Speed: 6.6ms preprocess, 1904.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.4ms\n",
      "Speed: 8.2ms preprocess, 2437.4ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2232.9ms\n",
      "Speed: 7.0ms preprocess, 2232.9ms inference, 7.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.6ms\n",
      "Speed: 8.1ms preprocess, 2229.6ms inference, 4.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1903.6ms\n",
      "Speed: 6.5ms preprocess, 1903.6ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.5ms\n",
      "Speed: 3.0ms preprocess, 1809.5ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 2291.5ms\n",
      "Speed: 6.5ms preprocess, 2291.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 2439.8ms\n",
      "Speed: 7.1ms preprocess, 2439.8ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 320x640 1 person, 1497.4ms\n",
      "Speed: 4.1ms preprocess, 1497.4ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 384x640 1 person, 1669.3ms\n",
      "Speed: 4.3ms preprocess, 1669.3ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.3ms\n",
      "Speed: 7.7ms preprocess, 2439.3ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1986.8ms\n",
      "Speed: 8.0ms preprocess, 1986.8ms inference, 4.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.2ms\n",
      "Speed: 6.2ms preprocess, 1907.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 2230.9ms\n",
      "Speed: 7.5ms preprocess, 2230.9ms inference, 5.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.9ms\n",
      "Speed: 6.2ms preprocess, 1905.9ms inference, 13.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.5ms\n",
      "Speed: 5.6ms preprocess, 1666.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 1773.0ms\n",
      "Speed: 5.5ms preprocess, 1773.0ms inference, 9.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.2ms\n",
      "Speed: 9.0ms preprocess, 1810.2ms inference, 8.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.5ms\n",
      "Speed: 8.1ms preprocess, 1908.5ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.1ms\n",
      "Speed: 6.1ms preprocess, 1666.1ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2441.7ms\n",
      "Speed: 8.5ms preprocess, 2441.7ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.6ms\n",
      "Speed: 6.5ms preprocess, 1905.6ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 1 person, 1764.6ms\n",
      "Speed: 4.8ms preprocess, 1764.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 352x640 1 person, 1588.6ms\n",
      "Speed: 6.5ms preprocess, 1588.6ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.9ms\n",
      "Speed: 8.3ms preprocess, 2439.9ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.8ms\n",
      "Speed: 6.5ms preprocess, 1811.8ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 6.1ms preprocess, 1809.9ms inference, 12.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1989.8ms\n",
      "Speed: 10.7ms preprocess, 1989.8ms inference, 16.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 person, 2298.4ms\n",
      "Speed: 12.9ms preprocess, 2298.4ms inference, 14.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 2445.6ms\n",
      "Speed: 21.9ms preprocess, 2445.6ms inference, 14.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1771.7ms\n",
      "Speed: 14.2ms preprocess, 1771.7ms inference, 6.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x608 1 person, 2361.7ms\n",
      "Speed: 10.1ms preprocess, 2361.7ms inference, 17.8ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x480 1 person, 1809.6ms\n",
      "Speed: 7.2ms preprocess, 1809.6ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x608 1 person, 2360.5ms\n",
      "Speed: 9.7ms preprocess, 2360.5ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 576x640 1 person, 2280.1ms\n",
      "Speed: 6.5ms preprocess, 2280.1ms inference, 6.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.2ms\n",
      "Speed: 6.5ms preprocess, 1813.2ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2448.0ms\n",
      "Speed: 9.0ms preprocess, 2448.0ms inference, 7.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.2ms\n",
      "Speed: 10.2ms preprocess, 1812.2ms inference, 8.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1909.9ms\n",
      "Speed: 7.8ms preprocess, 1909.9ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x544 1 person, 2291.9ms\n",
      "Speed: 6.4ms preprocess, 2291.9ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 2441.5ms\n",
      "Speed: 14.9ms preprocess, 2441.5ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.7ms\n",
      "Speed: 7.8ms preprocess, 2437.7ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.5ms\n",
      "Speed: 7.3ms preprocess, 2436.5ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2230.9ms\n",
      "Speed: 9.8ms preprocess, 2230.9ms inference, 7.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.5ms\n",
      "Speed: 6.1ms preprocess, 1664.5ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.4ms\n",
      "Speed: 8.9ms preprocess, 1904.4ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 1987.5ms\n",
      "Speed: 5.1ms preprocess, 1987.5ms inference, 5.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2441.3ms\n",
      "Speed: 9.9ms preprocess, 2441.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 person, 2280.3ms\n",
      "Speed: 8.5ms preprocess, 2280.3ms inference, 7.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 480x640 1 person, 1910.0ms\n",
      "Speed: 5.9ms preprocess, 1910.0ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.1ms\n",
      "Speed: 5.7ms preprocess, 1908.1ms inference, 6.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.8ms\n",
      "Speed: 5.6ms preprocess, 1813.8ms inference, 7.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.7ms\n",
      "Speed: 7.6ms preprocess, 2229.7ms inference, 5.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.6ms\n",
      "Speed: 4.5ms preprocess, 1664.6ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 2285.2ms\n",
      "Speed: 6.8ms preprocess, 2285.2ms inference, 7.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.3ms\n",
      "Speed: 7.3ms preprocess, 1665.3ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.4ms\n",
      "Speed: 8.4ms preprocess, 1666.4ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1816.4ms\n",
      "Speed: 13.5ms preprocess, 1816.4ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x608 1 person, 2359.1ms\n",
      "Speed: 16.7ms preprocess, 2359.1ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x448 1 person, 1765.9ms\n",
      "Speed: 4.5ms preprocess, 1765.9ms inference, 22.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 544x640 1 person, 2234.9ms\n",
      "Speed: 23.8ms preprocess, 2234.9ms inference, 6.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 512x640 1 person, 1986.4ms\n",
      "Speed: 5.8ms preprocess, 1986.4ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.8ms\n",
      "Speed: 5.1ms preprocess, 1811.8ms inference, 6.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 1572.8ms\n",
      "Speed: 7.0ms preprocess, 1572.8ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.4ms\n",
      "Speed: 4.7ms preprocess, 1667.4ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1910.7ms\n",
      "Speed: 14.8ms preprocess, 1910.7ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1772.9ms\n",
      "Speed: 5.1ms preprocess, 1772.9ms inference, 5.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1905.0ms\n",
      "Speed: 7.3ms preprocess, 1905.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.3ms\n",
      "Speed: 4.4ms preprocess, 1906.3ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.1ms\n",
      "Speed: 6.3ms preprocess, 1810.1ms inference, 14.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 480x640 1 person, 1908.0ms\n",
      "Speed: 7.5ms preprocess, 1908.0ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.3ms\n",
      "Speed: 4.5ms preprocess, 1813.3ms inference, 8.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1670.1ms\n",
      "Speed: 5.6ms preprocess, 1670.1ms inference, 10.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.1ms\n",
      "Speed: 8.7ms preprocess, 1908.1ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.8ms\n",
      "Speed: 7.7ms preprocess, 1809.8ms inference, 5.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1668.9ms\n",
      "Speed: 6.8ms preprocess, 1668.9ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.9ms\n",
      "Speed: 6.2ms preprocess, 1811.9ms inference, 5.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.2ms\n",
      "Speed: 6.7ms preprocess, 1809.2ms inference, 6.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1772.7ms\n",
      "Speed: 5.6ms preprocess, 1772.7ms inference, 5.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 7.5ms preprocess, 1809.9ms inference, 7.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1781.2ms\n",
      "Speed: 6.0ms preprocess, 1781.2ms inference, 6.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2456.8ms\n",
      "Speed: 9.6ms preprocess, 2456.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.9ms\n",
      "Speed: 6.2ms preprocess, 1812.9ms inference, 5.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.8ms\n",
      "Speed: 5.1ms preprocess, 1663.8ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.6ms\n",
      "Speed: 6.4ms preprocess, 1810.6ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.6ms\n",
      "Speed: 4.6ms preprocess, 1665.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1663.8ms\n",
      "Speed: 5.6ms preprocess, 1663.8ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.6ms\n",
      "Speed: 5.5ms preprocess, 1810.6ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.3ms\n",
      "Speed: 6.0ms preprocess, 1666.3ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 person, 1769.5ms\n",
      "Speed: 6.3ms preprocess, 1769.5ms inference, 5.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1805.7ms\n",
      "Speed: 6.4ms preprocess, 1805.7ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1805.7ms\n",
      "Speed: 5.5ms preprocess, 1805.7ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.8ms\n",
      "Speed: 7.4ms preprocess, 1807.8ms inference, 9.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.3ms\n",
      "Speed: 4.7ms preprocess, 1664.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.9ms\n",
      "Speed: 7.4ms preprocess, 1811.9ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.4ms\n",
      "Speed: 5.5ms preprocess, 1807.4ms inference, 9.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.9ms\n",
      "Speed: 6.7ms preprocess, 1906.9ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1903.5ms\n",
      "Speed: 5.7ms preprocess, 1903.5ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1805.6ms\n",
      "Speed: 6.5ms preprocess, 1805.6ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1773.4ms\n",
      "Speed: 5.9ms preprocess, 1773.4ms inference, 11.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 320x640 1 person, 1527.0ms\n",
      "Speed: 4.6ms preprocess, 1527.0ms inference, 8.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.9ms\n",
      "Speed: 7.5ms preprocess, 1812.9ms inference, 5.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.0ms\n",
      "Speed: 5.8ms preprocess, 1665.0ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.2ms\n",
      "Speed: 12.7ms preprocess, 2439.2ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2434.2ms\n",
      "Speed: 10.1ms preprocess, 2434.2ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1771.3ms\n",
      "Speed: 7.6ms preprocess, 1771.3ms inference, 7.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.7ms\n",
      "Speed: 6.8ms preprocess, 1809.7ms inference, 7.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1910.4ms\n",
      "Speed: 6.2ms preprocess, 1910.4ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1670.2ms\n",
      "Speed: 7.1ms preprocess, 1670.2ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.4ms\n",
      "Speed: 13.6ms preprocess, 2440.4ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.1ms\n",
      "Speed: 7.2ms preprocess, 1808.1ms inference, 5.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.6ms\n",
      "Speed: 7.1ms preprocess, 1807.6ms inference, 6.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 1496.6ms\n",
      "Speed: 4.9ms preprocess, 1496.6ms inference, 12.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 384x640 1 person, 1668.8ms\n",
      "Speed: 6.5ms preprocess, 1668.8ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.4ms\n",
      "Speed: 9.3ms preprocess, 1811.4ms inference, 5.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.5ms\n",
      "Speed: 7.3ms preprocess, 1984.5ms inference, 4.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.5ms\n",
      "Speed: 6.0ms preprocess, 1666.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.8ms\n",
      "Speed: 5.1ms preprocess, 1811.8ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.8ms\n",
      "Speed: 9.6ms preprocess, 1906.8ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1815.1ms\n",
      "Speed: 7.6ms preprocess, 1815.1ms inference, 7.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1909.2ms\n",
      "Speed: 6.3ms preprocess, 1909.2ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.9ms\n",
      "Speed: 7.8ms preprocess, 2438.9ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.7ms\n",
      "Speed: 6.8ms preprocess, 1813.7ms inference, 6.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1768.3ms\n",
      "Speed: 7.9ms preprocess, 1768.3ms inference, 5.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.7ms\n",
      "Speed: 5.6ms preprocess, 1664.7ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.5ms\n",
      "Speed: 4.3ms preprocess, 1665.5ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.2ms\n",
      "Speed: 6.3ms preprocess, 1666.2ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2442.6ms\n",
      "Speed: 7.8ms preprocess, 2442.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.7ms\n",
      "Speed: 7.0ms preprocess, 1812.7ms inference, 6.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.9ms\n",
      "Speed: 5.4ms preprocess, 1808.9ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1815.7ms\n",
      "Speed: 5.8ms preprocess, 1815.7ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.6ms\n",
      "Speed: 6.7ms preprocess, 1665.6ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.9ms\n",
      "Speed: 5.0ms preprocess, 1665.9ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1907.4ms\n",
      "Speed: 4.7ms preprocess, 1907.4ms inference, 9.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 (no detections), 1812.5ms\n",
      "Speed: 8.0ms preprocess, 1812.5ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.4ms\n",
      "Speed: 8.0ms preprocess, 1809.4ms inference, 5.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1669.1ms\n",
      "Speed: 3.9ms preprocess, 1669.1ms inference, 9.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.2ms\n",
      "Speed: 17.8ms preprocess, 1661.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.8ms\n",
      "Speed: 8.0ms preprocess, 1810.8ms inference, 12.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.7ms\n",
      "Speed: 9.8ms preprocess, 2438.7ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.0ms\n",
      "Speed: 6.6ms preprocess, 1666.0ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.9ms\n",
      "Speed: 7.7ms preprocess, 2440.9ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.1ms\n",
      "Speed: 6.5ms preprocess, 1904.1ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.4ms\n",
      "Speed: 7.0ms preprocess, 1904.4ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.6ms\n",
      "Speed: 5.2ms preprocess, 1665.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1815.4ms\n",
      "Speed: 5.8ms preprocess, 1815.4ms inference, 11.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1982.6ms\n",
      "Speed: 10.4ms preprocess, 1982.6ms inference, 4.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.8ms\n",
      "Speed: 5.8ms preprocess, 1810.8ms inference, 6.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.5ms\n",
      "Speed: 7.1ms preprocess, 2439.5ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.9ms\n",
      "Speed: 4.1ms preprocess, 1908.9ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1769.9ms\n",
      "Speed: 6.7ms preprocess, 1769.9ms inference, 7.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 2442.6ms\n",
      "Speed: 7.0ms preprocess, 2442.6ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.9ms\n",
      "Speed: 6.5ms preprocess, 1665.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.3ms\n",
      "Speed: 6.5ms preprocess, 1812.3ms inference, 12.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.1ms\n",
      "Speed: 6.4ms preprocess, 1666.1ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1906.1ms\n",
      "Speed: 8.5ms preprocess, 1906.1ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.4ms\n",
      "Speed: 3.8ms preprocess, 1667.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 person, 1876.2ms\n",
      "Speed: 5.5ms preprocess, 1876.2ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 1 person, 2440.8ms\n",
      "Speed: 7.7ms preprocess, 2440.8ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.0ms\n",
      "Speed: 5.0ms preprocess, 1812.0ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 1810.0ms\n",
      "Speed: 5.0ms preprocess, 1810.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1808.7ms\n",
      "Speed: 4.3ms preprocess, 1808.7ms inference, 5.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1985.8ms\n",
      "Speed: 8.4ms preprocess, 1985.8ms inference, 5.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.3ms\n",
      "Speed: 9.1ms preprocess, 2439.3ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.2ms\n",
      "Speed: 6.0ms preprocess, 2437.2ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.8ms\n",
      "Speed: 7.0ms preprocess, 2436.8ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.1ms\n",
      "Speed: 8.6ms preprocess, 2437.1ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1771.0ms\n",
      "Speed: 24.1ms preprocess, 1771.0ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1761.2ms\n",
      "Speed: 11.6ms preprocess, 1761.2ms inference, 12.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1813.0ms\n",
      "Speed: 7.5ms preprocess, 1813.0ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1761.9ms\n",
      "Speed: 8.5ms preprocess, 1761.9ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1759.8ms\n",
      "Speed: 7.3ms preprocess, 1759.8ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1760.6ms\n",
      "Speed: 5.4ms preprocess, 1760.6ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1759.7ms\n",
      "Speed: 5.6ms preprocess, 1759.7ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 576x640 1 person, 2283.6ms\n",
      "Speed: 8.3ms preprocess, 2283.6ms inference, 9.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x448 1 person, 1761.1ms\n",
      "Speed: 6.7ms preprocess, 1761.1ms inference, 13.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1667.2ms\n",
      "Speed: 7.3ms preprocess, 1667.2ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 1762.1ms\n",
      "Speed: 10.5ms preprocess, 1762.1ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2439.2ms\n",
      "Speed: 11.6ms preprocess, 2439.2ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1760.8ms\n",
      "Speed: 5.3ms preprocess, 1760.8ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1758.7ms\n",
      "Speed: 5.0ms preprocess, 1758.7ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2438.1ms\n",
      "Speed: 9.2ms preprocess, 2438.1ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.6ms\n",
      "Speed: 7.5ms preprocess, 1667.6ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1661.5ms\n",
      "Speed: 6.7ms preprocess, 1661.5ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1906.0ms\n",
      "Speed: 6.7ms preprocess, 1906.0ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1669.8ms\n",
      "Speed: 7.7ms preprocess, 1669.8ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.4ms\n",
      "Speed: 9.8ms preprocess, 2439.4ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.9ms\n",
      "Speed: 6.5ms preprocess, 1809.9ms inference, 5.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 1811.3ms\n",
      "Speed: 6.1ms preprocess, 1811.3ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 person, 1808.5ms\n",
      "Speed: 6.9ms preprocess, 1808.5ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 384x640 1 person, 1671.2ms\n",
      "Speed: 4.9ms preprocess, 1671.2ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 1 person, 2291.6ms\n",
      "Speed: 7.3ms preprocess, 2291.6ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 person, 2438.2ms\n",
      "Speed: 5.7ms preprocess, 2438.2ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.6ms\n",
      "Speed: 4.5ms preprocess, 1763.6ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1664.9ms\n",
      "Speed: 5.5ms preprocess, 1664.9ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 1762.9ms\n",
      "Speed: 6.0ms preprocess, 1762.9ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1762.0ms\n",
      "Speed: 5.4ms preprocess, 1762.0ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2437.8ms\n",
      "Speed: 10.1ms preprocess, 2437.8ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1814.9ms\n",
      "Speed: 17.2ms preprocess, 1814.9ms inference, 10.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1764.2ms\n",
      "Speed: 6.5ms preprocess, 1764.2ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 2443.9ms\n",
      "Speed: 16.1ms preprocess, 2443.9ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 person, 1881.3ms\n",
      "Speed: 13.0ms preprocess, 1881.3ms inference, 12.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 448x640 1 person, 1810.6ms\n",
      "Speed: 8.9ms preprocess, 1810.6ms inference, 6.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.6ms\n",
      "Speed: 8.6ms preprocess, 1809.6ms inference, 7.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 1 person, 1726.3ms\n",
      "Speed: 8.7ms preprocess, 1726.3ms inference, 10.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 person, 2450.0ms\n",
      "Speed: 14.2ms preprocess, 2450.0ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1577.0ms\n",
      "Speed: 13.0ms preprocess, 1577.0ms inference, 22.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x416 1 person, 1746.3ms\n",
      "Speed: 12.5ms preprocess, 1746.3ms inference, 53.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 1 person, 1831.2ms\n",
      "Speed: 31.8ms preprocess, 1831.2ms inference, 17.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2442.8ms\n",
      "Speed: 16.0ms preprocess, 2442.8ms inference, 18.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.5ms\n",
      "Speed: 12.9ms preprocess, 2438.5ms inference, 24.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.4ms\n",
      "Speed: 9.0ms preprocess, 1664.4ms inference, 9.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1910.4ms\n",
      "Speed: 36.9ms preprocess, 1910.4ms inference, 11.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1814.7ms\n",
      "Speed: 13.3ms preprocess, 1814.7ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2441.8ms\n",
      "Speed: 9.9ms preprocess, 2441.8ms inference, 9.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1808.2ms\n",
      "Speed: 7.2ms preprocess, 1808.2ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1773.4ms\n",
      "Speed: 7.5ms preprocess, 1773.4ms inference, 9.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 2444.4ms\n",
      "Speed: 8.4ms preprocess, 2444.4ms inference, 7.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.3ms\n",
      "Speed: 11.4ms preprocess, 1763.3ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x512 1 person, 1874.5ms\n",
      "Speed: 7.0ms preprocess, 1874.5ms inference, 7.6ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x416 3 persons, 1721.3ms\n",
      "Speed: 7.5ms preprocess, 1721.3ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 person, 2440.1ms\n",
      "Speed: 7.6ms preprocess, 2440.1ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.1ms\n",
      "Speed: 10.1ms preprocess, 2436.1ms inference, 9.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1762.7ms\n",
      "Speed: 7.7ms preprocess, 1762.7ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x544 1 person, 2288.5ms\n",
      "Speed: 7.9ms preprocess, 2288.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x448 1 person, 1763.7ms\n",
      "Speed: 5.1ms preprocess, 1763.7ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x544 1 person, 2291.9ms\n",
      "Speed: 5.0ms preprocess, 2291.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 1 person, 1670.5ms\n",
      "Speed: 5.0ms preprocess, 1670.5ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 1 person, 2280.1ms\n",
      "Speed: 6.2ms preprocess, 2280.1ms inference, 3.9ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x352 1 person, 1644.3ms\n",
      "Speed: 3.2ms preprocess, 1644.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x448 1 person, 1763.0ms\n",
      "Speed: 7.2ms preprocess, 1763.0ms inference, 13.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1760.6ms\n",
      "Speed: 5.0ms preprocess, 1760.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1759.7ms\n",
      "Speed: 4.0ms preprocess, 1759.7ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1810.7ms\n",
      "Speed: 3.8ms preprocess, 1810.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.5ms\n",
      "Speed: 7.5ms preprocess, 2437.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.5ms\n",
      "Speed: 4.6ms preprocess, 1665.5ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.2ms\n",
      "Speed: 6.6ms preprocess, 2439.2ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1666.3ms\n",
      "Speed: 3.9ms preprocess, 1666.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1813.9ms\n",
      "Speed: 5.3ms preprocess, 1813.9ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.5ms\n",
      "Speed: 4.9ms preprocess, 1807.5ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1981.7ms\n",
      "Speed: 6.2ms preprocess, 1981.7ms inference, 4.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.7ms\n",
      "Speed: 4.0ms preprocess, 1812.7ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2729.3ms\n",
      "Speed: 7.4ms preprocess, 2729.3ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1986.7ms\n",
      "Speed: 6.7ms preprocess, 1986.7ms inference, 3.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 576x640 1 person, 2279.9ms\n",
      "Speed: 6.2ms preprocess, 2279.9ms inference, 4.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 person, 2277.3ms\n",
      "Speed: 6.2ms preprocess, 2277.3ms inference, 5.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 544x640 1 person, 2229.2ms\n",
      "Speed: 5.0ms preprocess, 2229.2ms inference, 6.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.4ms\n",
      "Speed: 7.1ms preprocess, 1904.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1669.8ms\n",
      "Speed: 4.0ms preprocess, 1669.8ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x416 1 person, 1720.6ms\n",
      "Speed: 6.9ms preprocess, 1720.6ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 480x640 1 person, 1905.4ms\n",
      "Speed: 5.7ms preprocess, 1905.4ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 1985.0ms\n",
      "Speed: 5.4ms preprocess, 1985.0ms inference, 3.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.6ms\n",
      "Speed: 6.5ms preprocess, 1810.6ms inference, 6.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1805.6ms\n",
      "Speed: 5.1ms preprocess, 1805.6ms inference, 5.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1985.2ms\n",
      "Speed: 7.4ms preprocess, 1985.2ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 544x640 1 person, 2231.3ms\n",
      "Speed: 9.6ms preprocess, 2231.3ms inference, 5.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 2442.2ms\n",
      "Speed: 6.2ms preprocess, 2442.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1761.6ms\n",
      "Speed: 5.2ms preprocess, 1761.6ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 416x640 1 person, 1773.6ms\n",
      "Speed: 5.0ms preprocess, 1773.6ms inference, 6.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2438.1ms\n",
      "Speed: 7.6ms preprocess, 2438.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.7ms\n",
      "Speed: 5.5ms preprocess, 1807.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x512 1 person, 1872.0ms\n",
      "Speed: 8.0ms preprocess, 1872.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x416 1 person, 1719.6ms\n",
      "Speed: 6.0ms preprocess, 1719.6ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 512x640 1 person, 1980.9ms\n",
      "Speed: 7.0ms preprocess, 1980.9ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.4ms\n",
      "Speed: 3.8ms preprocess, 1809.4ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1807.2ms\n",
      "Speed: 5.5ms preprocess, 1807.2ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.5ms\n",
      "Speed: 6.5ms preprocess, 2439.5ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1770.7ms\n",
      "Speed: 5.8ms preprocess, 1770.7ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 576x640 1 person, 2280.5ms\n",
      "Speed: 6.2ms preprocess, 2280.5ms inference, 4.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 512x640 1 person, 1988.0ms\n",
      "Speed: 6.3ms preprocess, 1988.0ms inference, 15.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.1ms\n",
      "Speed: 9.2ms preprocess, 2440.1ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1571.3ms\n",
      "Speed: 4.0ms preprocess, 1571.3ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2443.5ms\n",
      "Speed: 8.4ms preprocess, 2443.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2233.2ms\n",
      "Speed: 7.5ms preprocess, 2233.2ms inference, 6.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x448 1 person, 1761.4ms\n",
      "Speed: 7.1ms preprocess, 1761.4ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1666.7ms\n",
      "Speed: 6.3ms preprocess, 1666.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.4ms\n",
      "Speed: 8.3ms preprocess, 2437.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1809.7ms\n",
      "Speed: 5.3ms preprocess, 1809.7ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2441.2ms\n",
      "Speed: 7.3ms preprocess, 2441.2ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.3ms\n",
      "Speed: 5.7ms preprocess, 1984.3ms inference, 3.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.0ms\n",
      "Speed: 7.5ms preprocess, 2437.0ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.2ms\n",
      "Speed: 9.0ms preprocess, 2436.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.6ms\n",
      "Speed: 6.9ms preprocess, 1904.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1665.4ms\n",
      "Speed: 6.5ms preprocess, 1665.4ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x608 1 person, 2357.5ms\n",
      "Speed: 6.1ms preprocess, 2357.5ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x416 1 person, 1720.6ms\n",
      "Speed: 6.1ms preprocess, 1720.6ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x448 1 person, 1761.6ms\n",
      "Speed: 6.6ms preprocess, 1761.6ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1811.2ms\n",
      "Speed: 6.5ms preprocess, 1811.2ms inference, 7.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 1 person, 2230.7ms\n",
      "Speed: 5.0ms preprocess, 2230.7ms inference, 5.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 352x640 1 person, 1575.1ms\n",
      "Speed: 4.1ms preprocess, 1575.1ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.9ms\n",
      "Speed: 6.0ms preprocess, 1905.9ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 2436.7ms\n",
      "Speed: 8.0ms preprocess, 2436.7ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.3ms\n",
      "Speed: 4.1ms preprocess, 1812.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.2ms\n",
      "Speed: 7.3ms preprocess, 2439.2ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.2ms\n",
      "Speed: 8.3ms preprocess, 2437.2ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.2ms\n",
      "Speed: 5.3ms preprocess, 1810.2ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1763.3ms\n",
      "Speed: 5.3ms preprocess, 1763.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 1907.7ms\n",
      "Speed: 6.9ms preprocess, 1907.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x608 1 person, 2361.5ms\n",
      "Speed: 8.4ms preprocess, 2361.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 1 person, 1669.8ms\n",
      "Speed: 5.0ms preprocess, 1669.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 person, 1872.3ms\n",
      "Speed: 8.0ms preprocess, 1872.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 480x640 1 person, 1906.1ms\n",
      "Speed: 4.0ms preprocess, 1906.1ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.0ms\n",
      "Speed: 5.0ms preprocess, 1984.0ms inference, 4.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 352x640 1 person, 1570.6ms\n",
      "Speed: 5.0ms preprocess, 1570.6ms inference, 3.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 512x640 1 person, 1987.0ms\n",
      "Speed: 5.1ms preprocess, 1987.0ms inference, 4.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x608 1 person, 2357.6ms\n",
      "Speed: 8.2ms preprocess, 2357.6ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 448x640 1 person, 1812.0ms\n",
      "Speed: 5.1ms preprocess, 1812.0ms inference, 3.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.3ms\n",
      "Speed: 8.8ms preprocess, 1984.3ms inference, 7.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 2437.9ms\n",
      "Speed: 6.7ms preprocess, 2437.9ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 2230.4ms\n",
      "Speed: 5.0ms preprocess, 2230.4ms inference, 3.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 480x640 1 person, 1905.0ms\n",
      "Speed: 5.3ms preprocess, 1905.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.1ms\n",
      "Speed: 5.9ms preprocess, 1904.1ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 1571.8ms\n",
      "Speed: 7.9ms preprocess, 1571.8ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 1 person, 1908.5ms\n",
      "Speed: 8.1ms preprocess, 1908.5ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 1810.3ms\n",
      "Speed: 11.0ms preprocess, 1810.3ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 544x640 1 person, 2233.6ms\n",
      "Speed: 8.0ms preprocess, 2233.6ms inference, 7.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x416 1 person, 1718.5ms\n",
      "Speed: 8.0ms preprocess, 1718.5ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x448 1 person, 1764.0ms\n",
      "Speed: 6.7ms preprocess, 1764.0ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 1668.2ms\n",
      "Speed: 6.3ms preprocess, 1668.2ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1904.2ms\n",
      "Speed: 8.6ms preprocess, 1904.2ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.6ms\n",
      "Speed: 6.5ms preprocess, 1810.6ms inference, 3.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x608 1 person, 2361.7ms\n",
      "Speed: 6.0ms preprocess, 2361.7ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 448x640 1 person, 1812.8ms\n",
      "Speed: 5.4ms preprocess, 1812.8ms inference, 5.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 224x640 1 person, 1102.1ms\n",
      "Speed: 3.7ms preprocess, 1102.1ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.1ms\n",
      "Speed: 6.1ms preprocess, 1812.1ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.4ms\n",
      "Speed: 9.0ms preprocess, 2438.4ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2435.3ms\n",
      "Speed: 6.1ms preprocess, 2435.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2438.2ms\n",
      "Speed: 9.0ms preprocess, 2438.2ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 1 person, 1724.0ms\n",
      "Speed: 6.0ms preprocess, 1724.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 1 person, 1812.3ms\n",
      "Speed: 6.3ms preprocess, 1812.3ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1810.7ms\n",
      "Speed: 4.4ms preprocess, 1810.7ms inference, 5.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.1ms\n",
      "Speed: 6.3ms preprocess, 2439.1ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1984.8ms\n",
      "Speed: 6.3ms preprocess, 1984.8ms inference, 4.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1664.6ms\n",
      "Speed: 6.1ms preprocess, 1664.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.7ms\n",
      "Speed: 6.0ms preprocess, 1983.7ms inference, 5.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x512 1 person, 1871.9ms\n",
      "Speed: 6.0ms preprocess, 1871.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 512x640 1 person, 1987.5ms\n",
      "Speed: 16.6ms preprocess, 1987.5ms inference, 4.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 1667.1ms\n",
      "Speed: 6.0ms preprocess, 1667.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1812.7ms\n",
      "Speed: 5.4ms preprocess, 1812.7ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2439.5ms\n",
      "Speed: 8.4ms preprocess, 2439.5ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2440.2ms\n",
      "Speed: 11.2ms preprocess, 2440.2ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1983.6ms\n",
      "Speed: 5.0ms preprocess, 1983.6ms inference, 9.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 1811.3ms\n",
      "Speed: 7.0ms preprocess, 1811.3ms inference, 8.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "200\n",
      "253\n",
      "279\n",
      "378\n",
      "391\n"
     ]
    }
   ],
   "source": [
    "test_data = process_dataset(r\"D:\\archith\\DATASET\\TEST\")\n",
    "\n",
    "test_data_3d = []\n",
    "count = 0\n",
    "for i in test_data:\n",
    "    if(len(i[0][0]) == 17):\n",
    "        test_data_3d.append(i[0][0]) \n",
    "    else: print(count)\n",
    "    count+=1\n",
    "test_data = np.array(test_data_3d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cab362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = [1]*208\n",
    "label2 = [2]*177\n",
    "label3 = [3]*263\n",
    "label4 = [4]*160\n",
    "label5 = [5]*240\n",
    "label = label1+label2+label3+label4+label5\n",
    "labels = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "674bb489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:27:27.733019Z",
     "iopub.status.busy": "2024-07-12T13:27:27.732384Z",
     "iopub.status.idle": "2024-07-12T13:27:27.739947Z",
     "shell.execute_reply": "2024-07-12T13:27:27.739083Z"
    },
    "papermill": {
     "duration": 0.398203,
     "end_time": "2024-07-12T13:27:27.741976",
     "exception": false,
     "start_time": "2024-07-12T13:27:27.343773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.1107 - loss: 2.2518\n",
      "Epoch 2/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1603 - loss: 2.2190\n",
      "Epoch 3/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1994 - loss: 2.1879\n",
      "Epoch 4/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2147 - loss: 2.1582\n",
      "Epoch 5/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2271 - loss: 2.1298\n",
      "Epoch 6/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2281 - loss: 2.1031\n",
      "Epoch 7/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2309 - loss: 2.0780\n",
      "Epoch 8/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2328 - loss: 2.0545\n",
      "Epoch 9/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2290 - loss: 2.0320\n",
      "Epoch 10/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2300 - loss: 2.0103\n",
      "Epoch 11/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2300 - loss: 1.9891\n",
      "Epoch 12/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2328 - loss: 1.9684\n",
      "Epoch 13/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2357 - loss: 1.9482\n",
      "Epoch 14/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2366 - loss: 1.9282\n",
      "Epoch 15/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2376 - loss: 1.9085\n",
      "Epoch 16/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2395 - loss: 1.8890\n",
      "Epoch 17/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2405 - loss: 1.8694\n",
      "Epoch 18/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2462 - loss: 1.8498\n",
      "Epoch 19/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2567 - loss: 1.8302\n",
      "Epoch 20/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2729 - loss: 1.8105\n",
      "Epoch 21/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2929 - loss: 1.7910\n",
      "Epoch 22/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3197 - loss: 1.7718\n",
      "Epoch 23/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3397 - loss: 1.7529\n",
      "Epoch 24/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3540 - loss: 1.7339\n",
      "Epoch 25/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3693 - loss: 1.7147\n",
      "Epoch 26/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3798 - loss: 1.6953\n",
      "Epoch 27/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3989 - loss: 1.6759\n",
      "Epoch 28/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4141 - loss: 1.6564\n",
      "Epoch 29/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4208 - loss: 1.6366\n",
      "Epoch 30/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4208 - loss: 1.6164\n",
      "Epoch 31/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4284 - loss: 1.5952\n",
      "Epoch 32/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4323 - loss: 1.5739\n",
      "Epoch 33/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4361 - loss: 1.5534\n",
      "Epoch 34/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4418 - loss: 1.5338\n",
      "Epoch 35/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4456 - loss: 1.5153\n",
      "Epoch 36/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4494 - loss: 1.4978\n",
      "Epoch 37/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4618 - loss: 1.4811\n",
      "Epoch 38/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4800 - loss: 1.4651\n",
      "Epoch 39/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4885 - loss: 1.4498\n",
      "Epoch 40/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5067 - loss: 1.4351\n",
      "Epoch 41/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5134 - loss: 1.4208\n",
      "Epoch 42/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5162 - loss: 1.4069\n",
      "Epoch 43/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5067 - loss: 1.3934\n",
      "Epoch 44/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4990 - loss: 1.3804\n",
      "Epoch 45/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4819 - loss: 1.3677\n",
      "Epoch 46/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4781 - loss: 1.3554\n",
      "Epoch 47/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4742 - loss: 1.3436\n",
      "Epoch 48/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4714 - loss: 1.3323\n",
      "Epoch 49/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4723 - loss: 1.3213\n",
      "Epoch 50/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4704 - loss: 1.3107\n",
      "Epoch 51/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4733 - loss: 1.3004\n",
      "Epoch 52/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4733 - loss: 1.2903\n",
      "Epoch 53/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4828 - loss: 1.2805\n",
      "Epoch 54/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4866 - loss: 1.2710\n",
      "Epoch 55/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4895 - loss: 1.2617\n",
      "Epoch 56/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4876 - loss: 1.2529\n",
      "Epoch 57/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4914 - loss: 1.2445\n",
      "Epoch 58/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4933 - loss: 1.2364\n",
      "Epoch 59/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5029 - loss: 1.2286\n",
      "Epoch 60/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5076 - loss: 1.2209\n",
      "Epoch 61/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5248 - loss: 1.2134\n",
      "Epoch 62/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5305 - loss: 1.2059\n",
      "Epoch 63/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5286 - loss: 1.1984\n",
      "Epoch 64/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5229 - loss: 1.1910\n",
      "Epoch 65/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5124 - loss: 1.1837\n",
      "Epoch 66/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5076 - loss: 1.1767\n",
      "Epoch 67/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5029 - loss: 1.1698\n",
      "Epoch 68/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5019 - loss: 1.1630\n",
      "Epoch 69/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5048 - loss: 1.1564\n",
      "Epoch 70/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5105 - loss: 1.1499\n",
      "Epoch 71/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5143 - loss: 1.1434\n",
      "Epoch 72/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5172 - loss: 1.1369\n",
      "Epoch 73/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5210 - loss: 1.1306\n",
      "Epoch 74/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5296 - loss: 1.1243\n",
      "Epoch 75/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5401 - loss: 1.1182\n",
      "Epoch 76/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5506 - loss: 1.1122\n",
      "Epoch 77/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5544 - loss: 1.1063\n",
      "Epoch 78/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5573 - loss: 1.1004\n",
      "Epoch 79/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5592 - loss: 1.0946\n",
      "Epoch 80/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5563 - loss: 1.0888\n",
      "Epoch 81/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5553 - loss: 1.0830\n",
      "Epoch 82/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5534 - loss: 1.0774\n",
      "Epoch 83/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5563 - loss: 1.0719\n",
      "Epoch 84/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5573 - loss: 1.0664\n",
      "Epoch 85/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5601 - loss: 1.0610\n",
      "Epoch 86/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5639 - loss: 1.0556\n",
      "Epoch 87/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5658 - loss: 1.0503\n",
      "Epoch 88/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5697 - loss: 1.0450\n",
      "Epoch 89/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5792 - loss: 1.0399\n",
      "Epoch 90/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5830 - loss: 1.0348\n",
      "Epoch 91/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5878 - loss: 1.0297\n",
      "Epoch 92/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5897 - loss: 1.0246\n",
      "Epoch 93/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5954 - loss: 1.0195\n",
      "Epoch 94/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5973 - loss: 1.0145\n",
      "Epoch 95/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5992 - loss: 1.0095\n",
      "Epoch 96/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6031 - loss: 1.0045\n",
      "Epoch 97/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6040 - loss: 0.9996\n",
      "Epoch 98/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6078 - loss: 0.9948\n",
      "Epoch 99/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6097 - loss: 0.9899\n",
      "Epoch 100/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6155 - loss: 0.9851\n",
      "Epoch 101/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6202 - loss: 0.9803\n",
      "Epoch 102/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6212 - loss: 0.9755\n",
      "Epoch 103/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6183 - loss: 0.9708\n",
      "Epoch 104/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6174 - loss: 0.9660\n",
      "Epoch 105/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6183 - loss: 0.9613\n",
      "Epoch 106/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6202 - loss: 0.9567\n",
      "Epoch 107/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6260 - loss: 0.9523\n",
      "Epoch 108/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6336 - loss: 0.9478\n",
      "Epoch 109/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6336 - loss: 0.9434\n",
      "Epoch 110/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6374 - loss: 0.9389\n",
      "Epoch 111/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6441 - loss: 0.9344\n",
      "Epoch 112/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6508 - loss: 0.9300\n",
      "Epoch 113/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6546 - loss: 0.9255\n",
      "Epoch 114/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6584 - loss: 0.9211\n",
      "Epoch 115/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6622 - loss: 0.9168\n",
      "Epoch 116/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6698 - loss: 0.9125\n",
      "Epoch 117/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6718 - loss: 0.9083\n",
      "Epoch 118/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6813 - loss: 0.9040\n",
      "Epoch 119/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6899 - loss: 0.8998\n",
      "Epoch 120/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6927 - loss: 0.8956\n",
      "Epoch 121/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6975 - loss: 0.8913\n",
      "Epoch 122/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7061 - loss: 0.8871\n",
      "Epoch 123/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7080 - loss: 0.8829\n",
      "Epoch 124/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7109 - loss: 0.8788\n",
      "Epoch 125/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7147 - loss: 0.8747\n",
      "Epoch 126/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7156 - loss: 0.8706\n",
      "Epoch 127/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7204 - loss: 0.8665\n",
      "Epoch 128/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7242 - loss: 0.8624\n",
      "Epoch 129/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7300 - loss: 0.8584\n",
      "Epoch 130/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7347 - loss: 0.8544\n",
      "Epoch 131/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7366 - loss: 0.8503\n",
      "Epoch 132/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7395 - loss: 0.8464\n",
      "Epoch 133/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7424 - loss: 0.8424\n",
      "Epoch 134/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7481 - loss: 0.8384\n",
      "Epoch 135/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7567 - loss: 0.8345\n",
      "Epoch 136/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7567 - loss: 0.8305\n",
      "Epoch 137/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7624 - loss: 0.8266\n",
      "Epoch 138/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7634 - loss: 0.8227\n",
      "Epoch 139/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7634 - loss: 0.8188\n",
      "Epoch 140/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7653 - loss: 0.8149\n",
      "Epoch 141/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7653 - loss: 0.8111\n",
      "Epoch 142/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7672 - loss: 0.8072\n",
      "Epoch 143/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7672 - loss: 0.8034\n",
      "Epoch 144/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7691 - loss: 0.7996\n",
      "Epoch 145/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7719 - loss: 0.7958\n",
      "Epoch 146/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7739 - loss: 0.7920\n",
      "Epoch 147/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7767 - loss: 0.7882\n",
      "Epoch 148/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7777 - loss: 0.7844\n",
      "Epoch 149/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7796 - loss: 0.7805\n",
      "Epoch 150/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7824 - loss: 0.7767\n",
      "Epoch 151/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7853 - loss: 0.7727\n",
      "Epoch 152/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7844 - loss: 0.7687\n",
      "Epoch 153/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7815 - loss: 0.7647\n",
      "Epoch 154/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7834 - loss: 0.7609\n",
      "Epoch 155/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7872 - loss: 0.7572\n",
      "Epoch 156/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7901 - loss: 0.7536\n",
      "Epoch 157/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7910 - loss: 0.7499\n",
      "Epoch 158/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7948 - loss: 0.7463\n",
      "Epoch 159/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7948 - loss: 0.7427\n",
      "Epoch 160/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7968 - loss: 0.7391\n",
      "Epoch 161/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7996 - loss: 0.7356\n",
      "Epoch 162/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8025 - loss: 0.7320\n",
      "Epoch 163/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8015 - loss: 0.7285\n",
      "Epoch 164/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8025 - loss: 0.7250\n",
      "Epoch 165/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8034 - loss: 0.7216\n",
      "Epoch 166/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8034 - loss: 0.7183\n",
      "Epoch 167/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8063 - loss: 0.7149\n",
      "Epoch 168/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8073 - loss: 0.7116\n",
      "Epoch 169/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8082 - loss: 0.7083\n",
      "Epoch 170/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8111 - loss: 0.7050\n",
      "Epoch 171/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8139 - loss: 0.7017\n",
      "Epoch 172/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8197 - loss: 0.6985\n",
      "Epoch 173/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8206 - loss: 0.6953\n",
      "Epoch 174/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8235 - loss: 0.6921\n",
      "Epoch 175/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8254 - loss: 0.6890\n",
      "Epoch 176/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8263 - loss: 0.6859\n",
      "Epoch 177/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8273 - loss: 0.6828\n",
      "Epoch 178/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8311 - loss: 0.6797\n",
      "Epoch 179/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8311 - loss: 0.6767\n",
      "Epoch 180/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8321 - loss: 0.6737\n",
      "Epoch 181/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8321 - loss: 0.6706\n",
      "Epoch 182/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8321 - loss: 0.6676\n",
      "Epoch 183/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8321 - loss: 0.6645\n",
      "Epoch 184/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8311 - loss: 0.6616\n",
      "Epoch 185/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8292 - loss: 0.6587\n",
      "Epoch 186/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8321 - loss: 0.6559\n",
      "Epoch 187/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8368 - loss: 0.6531\n",
      "Epoch 188/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8368 - loss: 0.6503\n",
      "Epoch 189/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8349 - loss: 0.6476\n",
      "Epoch 190/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8359 - loss: 0.6449\n",
      "Epoch 191/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8359 - loss: 0.6421\n",
      "Epoch 192/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8378 - loss: 0.6394\n",
      "Epoch 193/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8378 - loss: 0.6367\n",
      "Epoch 194/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8378 - loss: 0.6341\n",
      "Epoch 195/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8406 - loss: 0.6315\n",
      "Epoch 196/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8406 - loss: 0.6289\n",
      "Epoch 197/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8406 - loss: 0.6264\n",
      "Epoch 198/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8406 - loss: 0.6239\n",
      "Epoch 199/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8406 - loss: 0.6214\n",
      "Epoch 200/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8406 - loss: 0.6190\n",
      "Epoch 201/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8387 - loss: 0.6165\n",
      "Epoch 202/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8397 - loss: 0.6141\n",
      "Epoch 203/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8378 - loss: 0.6117\n",
      "Epoch 204/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8378 - loss: 0.6094\n",
      "Epoch 205/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8378 - loss: 0.6071\n",
      "Epoch 206/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8406 - loss: 0.6048\n",
      "Epoch 207/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8435 - loss: 0.6025\n",
      "Epoch 208/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8454 - loss: 0.6003\n",
      "Epoch 209/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8454 - loss: 0.5981\n",
      "Epoch 210/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8464 - loss: 0.5959\n",
      "Epoch 211/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8464 - loss: 0.5938\n",
      "Epoch 212/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8483 - loss: 0.5916\n",
      "Epoch 213/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8492 - loss: 0.5895\n",
      "Epoch 214/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8483 - loss: 0.5874\n",
      "Epoch 215/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8483 - loss: 0.5853\n",
      "Epoch 216/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8483 - loss: 0.5833\n",
      "Epoch 217/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8483 - loss: 0.5812\n",
      "Epoch 218/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8492 - loss: 0.5792\n",
      "Epoch 219/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8502 - loss: 0.5773\n",
      "Epoch 220/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8502 - loss: 0.5753\n",
      "Epoch 221/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8521 - loss: 0.5734\n",
      "Epoch 222/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8492 - loss: 0.5715\n",
      "Epoch 223/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8492 - loss: 0.5696\n",
      "Epoch 224/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8502 - loss: 0.5677\n",
      "Epoch 225/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8502 - loss: 0.5658\n",
      "Epoch 226/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8531 - loss: 0.5640\n",
      "Epoch 227/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8531 - loss: 0.5622\n",
      "Epoch 228/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8531 - loss: 0.5604\n",
      "Epoch 229/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8540 - loss: 0.5586\n",
      "Epoch 230/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8559 - loss: 0.5568\n",
      "Epoch 231/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8559 - loss: 0.5550\n",
      "Epoch 232/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8559 - loss: 0.5533\n",
      "Epoch 233/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8559 - loss: 0.5516\n",
      "Epoch 234/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8559 - loss: 0.5499\n",
      "Epoch 235/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8559 - loss: 0.5481\n",
      "Epoch 236/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8559 - loss: 0.5465\n",
      "Epoch 237/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8559 - loss: 0.5448\n",
      "Epoch 238/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8559 - loss: 0.5431\n",
      "Epoch 239/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8559 - loss: 0.5415\n",
      "Epoch 240/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8578 - loss: 0.5398\n",
      "Epoch 241/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8578 - loss: 0.5382\n",
      "Epoch 242/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8578 - loss: 0.5366\n",
      "Epoch 243/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8588 - loss: 0.5350\n",
      "Epoch 244/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8597 - loss: 0.5334\n",
      "Epoch 245/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8597 - loss: 0.5318\n",
      "Epoch 246/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8597 - loss: 0.5302\n",
      "Epoch 247/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8597 - loss: 0.5287\n",
      "Epoch 248/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8597 - loss: 0.5271\n",
      "Epoch 249/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8597 - loss: 0.5256\n",
      "Epoch 250/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8597 - loss: 0.5240\n",
      "Epoch 251/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8607 - loss: 0.5225\n",
      "Epoch 252/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8626 - loss: 0.5210\n",
      "Epoch 253/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8626 - loss: 0.5195\n",
      "Epoch 254/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8626 - loss: 0.5180\n",
      "Epoch 255/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8645 - loss: 0.5165\n",
      "Epoch 256/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8655 - loss: 0.5150\n",
      "Epoch 257/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8655 - loss: 0.5135\n",
      "Epoch 258/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8655 - loss: 0.5120\n",
      "Epoch 259/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8655 - loss: 0.5105\n",
      "Epoch 260/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8674 - loss: 0.5091\n",
      "Epoch 261/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8674 - loss: 0.5076\n",
      "Epoch 262/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8674 - loss: 0.5062\n",
      "Epoch 263/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8674 - loss: 0.5047\n",
      "Epoch 264/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8674 - loss: 0.5033\n",
      "Epoch 265/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8674 - loss: 0.5018\n",
      "Epoch 266/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8674 - loss: 0.5004\n",
      "Epoch 267/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8674 - loss: 0.4990\n",
      "Epoch 268/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8683 - loss: 0.4976\n",
      "Epoch 269/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8683 - loss: 0.4962\n",
      "Epoch 270/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8674 - loss: 0.4948\n",
      "Epoch 271/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8693 - loss: 0.4935\n",
      "Epoch 272/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8693 - loss: 0.4921\n",
      "Epoch 273/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8712 - loss: 0.4907\n",
      "Epoch 274/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8712 - loss: 0.4893\n",
      "Epoch 275/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8712 - loss: 0.4879\n",
      "Epoch 276/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8712 - loss: 0.4866\n",
      "Epoch 277/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8712 - loss: 0.4853\n",
      "Epoch 278/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8702 - loss: 0.4839\n",
      "Epoch 279/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8702 - loss: 0.4825\n",
      "Epoch 280/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8702 - loss: 0.4812\n",
      "Epoch 281/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8712 - loss: 0.4799\n",
      "Epoch 282/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8712 - loss: 0.4787\n",
      "Epoch 283/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8731 - loss: 0.4774\n",
      "Epoch 284/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8731 - loss: 0.4761\n",
      "Epoch 285/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8731 - loss: 0.4749\n",
      "Epoch 286/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8740 - loss: 0.4737\n",
      "Epoch 287/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8750 - loss: 0.4724\n",
      "Epoch 288/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8750 - loss: 0.4712\n",
      "Epoch 289/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8760 - loss: 0.4700\n",
      "Epoch 290/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8760 - loss: 0.4688\n",
      "Epoch 291/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8760 - loss: 0.4676\n",
      "Epoch 292/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8760 - loss: 0.4664\n",
      "Epoch 293/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8760 - loss: 0.4652\n",
      "Epoch 294/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8779 - loss: 0.4640\n",
      "Epoch 295/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8798 - loss: 0.4628\n",
      "Epoch 296/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8798 - loss: 0.4616\n",
      "Epoch 297/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8769 - loss: 0.4604\n",
      "Epoch 298/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8788 - loss: 0.4592\n",
      "Epoch 299/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8798 - loss: 0.4581\n",
      "Epoch 300/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8798 - loss: 0.4569\n",
      "Epoch 301/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8788 - loss: 0.4557\n",
      "Epoch 302/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8788 - loss: 0.4546\n",
      "Epoch 303/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8807 - loss: 0.4534\n",
      "Epoch 304/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8807 - loss: 0.4522\n",
      "Epoch 305/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8807 - loss: 0.4511\n",
      "Epoch 306/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8807 - loss: 0.4499\n",
      "Epoch 307/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.4488\n",
      "Epoch 308/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8798 - loss: 0.4477\n",
      "Epoch 309/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8798 - loss: 0.4465\n",
      "Epoch 310/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8807 - loss: 0.4454\n",
      "Epoch 311/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8807 - loss: 0.4443\n",
      "Epoch 312/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8807 - loss: 0.4432\n",
      "Epoch 313/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8807 - loss: 0.4421\n",
      "Epoch 314/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8807 - loss: 0.4410\n",
      "Epoch 315/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.4399\n",
      "Epoch 316/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8807 - loss: 0.4388\n",
      "Epoch 317/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.4377\n",
      "Epoch 318/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8817 - loss: 0.4366\n",
      "Epoch 319/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8817 - loss: 0.4355\n",
      "Epoch 320/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8817 - loss: 0.4345\n",
      "Epoch 321/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8807 - loss: 0.4334\n",
      "Epoch 322/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.4323\n",
      "Epoch 323/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8807 - loss: 0.4312\n",
      "Epoch 324/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8807 - loss: 0.4302\n",
      "Epoch 325/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8807 - loss: 0.4291\n",
      "Epoch 326/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8807 - loss: 0.4280\n",
      "Epoch 327/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8798 - loss: 0.4270\n",
      "Epoch 328/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8807 - loss: 0.4259\n",
      "Epoch 329/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8836 - loss: 0.4249\n",
      "Epoch 330/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8836 - loss: 0.4239\n",
      "Epoch 331/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8836 - loss: 0.4228\n",
      "Epoch 332/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8836 - loss: 0.4218\n",
      "Epoch 333/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8836 - loss: 0.4208\n",
      "Epoch 334/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8836 - loss: 0.4198\n",
      "Epoch 335/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8836 - loss: 0.4187\n",
      "Epoch 336/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8836 - loss: 0.4177\n",
      "Epoch 337/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8836 - loss: 0.4167\n",
      "Epoch 338/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8836 - loss: 0.4157\n",
      "Epoch 339/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8836 - loss: 0.4147\n",
      "Epoch 340/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8836 - loss: 0.4137\n",
      "Epoch 341/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8836 - loss: 0.4127\n",
      "Epoch 342/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8836 - loss: 0.4117\n",
      "Epoch 343/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8836 - loss: 0.4108\n",
      "Epoch 344/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8836 - loss: 0.4098\n",
      "Epoch 345/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8836 - loss: 0.4088\n",
      "Epoch 346/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8845 - loss: 0.4079\n",
      "Epoch 347/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8845 - loss: 0.4069\n",
      "Epoch 348/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8845 - loss: 0.4059\n",
      "Epoch 349/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8865 - loss: 0.4050\n",
      "Epoch 350/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8865 - loss: 0.4040\n",
      "Epoch 351/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8865 - loss: 0.4031\n",
      "Epoch 352/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8865 - loss: 0.4022\n",
      "Epoch 353/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8865 - loss: 0.4012\n",
      "Epoch 354/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8865 - loss: 0.4003\n",
      "Epoch 355/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8865 - loss: 0.3994\n",
      "Epoch 356/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8865 - loss: 0.3984\n",
      "Epoch 357/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8865 - loss: 0.3975\n",
      "Epoch 358/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8865 - loss: 0.3966\n",
      "Epoch 359/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8884 - loss: 0.3957\n",
      "Epoch 360/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8893 - loss: 0.3948\n",
      "Epoch 361/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8903 - loss: 0.3939\n",
      "Epoch 362/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8893 - loss: 0.3930\n",
      "Epoch 363/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3921\n",
      "Epoch 364/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3912\n",
      "Epoch 365/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8903 - loss: 0.3903\n",
      "Epoch 366/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8912 - loss: 0.3894\n",
      "Epoch 367/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8903 - loss: 0.3885\n",
      "Epoch 368/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8903 - loss: 0.3877\n",
      "Epoch 369/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3868\n",
      "Epoch 370/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3859\n",
      "Epoch 371/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8903 - loss: 0.3851\n",
      "Epoch 372/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8893 - loss: 0.3842\n",
      "Epoch 373/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8893 - loss: 0.3834\n",
      "Epoch 374/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8893 - loss: 0.3826\n",
      "Epoch 375/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3817\n",
      "Epoch 376/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8903 - loss: 0.3809\n",
      "Epoch 377/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8903 - loss: 0.3800\n",
      "Epoch 378/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8903 - loss: 0.3792\n",
      "Epoch 379/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3783\n",
      "Epoch 380/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8903 - loss: 0.3775\n",
      "Epoch 381/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8903 - loss: 0.3767\n",
      "Epoch 382/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8903 - loss: 0.3758\n",
      "Epoch 383/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8903 - loss: 0.3750\n",
      "Epoch 384/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8903 - loss: 0.3742\n",
      "Epoch 385/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3734\n",
      "Epoch 386/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3725\n",
      "Epoch 387/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8912 - loss: 0.3717\n",
      "Epoch 388/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8912 - loss: 0.3709\n",
      "Epoch 389/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8912 - loss: 0.3701\n",
      "Epoch 390/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8903 - loss: 0.3692\n",
      "Epoch 391/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8912 - loss: 0.3684\n",
      "Epoch 392/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8912 - loss: 0.3676\n",
      "Epoch 393/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8912 - loss: 0.3668\n",
      "Epoch 394/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8903 - loss: 0.3660\n",
      "Epoch 395/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8931 - loss: 0.3651\n",
      "Epoch 396/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8931 - loss: 0.3643\n",
      "Epoch 397/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8931 - loss: 0.3635\n",
      "Epoch 398/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8931 - loss: 0.3627\n",
      "Epoch 399/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8931 - loss: 0.3619\n",
      "Epoch 400/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8931 - loss: 0.3611\n",
      "Epoch 401/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8931 - loss: 0.3603\n",
      "Epoch 402/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8931 - loss: 0.3595\n",
      "Epoch 403/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8931 - loss: 0.3587\n",
      "Epoch 404/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8941 - loss: 0.3579\n",
      "Epoch 405/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8931 - loss: 0.3571\n",
      "Epoch 406/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8941 - loss: 0.3563\n",
      "Epoch 407/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8941 - loss: 0.3555\n",
      "Epoch 408/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8941 - loss: 0.3547\n",
      "Epoch 409/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8941 - loss: 0.3539\n",
      "Epoch 410/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8941 - loss: 0.3531\n",
      "Epoch 411/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8931 - loss: 0.3523\n",
      "Epoch 412/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8931 - loss: 0.3516\n",
      "Epoch 413/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8950 - loss: 0.3508\n",
      "Epoch 414/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8960 - loss: 0.3500\n",
      "Epoch 415/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8950 - loss: 0.3493\n",
      "Epoch 416/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8950 - loss: 0.3485\n",
      "Epoch 417/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8969 - loss: 0.3477\n",
      "Epoch 418/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8969 - loss: 0.3470\n",
      "Epoch 419/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8969 - loss: 0.3462\n",
      "Epoch 420/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8969 - loss: 0.3455\n",
      "Epoch 421/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8969 - loss: 0.3447\n",
      "Epoch 422/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8969 - loss: 0.3439\n",
      "Epoch 423/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8969 - loss: 0.3432\n",
      "Epoch 424/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8979 - loss: 0.3424\n",
      "Epoch 425/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8979 - loss: 0.3417\n",
      "Epoch 426/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8979 - loss: 0.3409\n",
      "Epoch 427/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8998 - loss: 0.3402\n",
      "Epoch 428/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8998 - loss: 0.3394\n",
      "Epoch 429/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8998 - loss: 0.3387\n",
      "Epoch 430/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9008 - loss: 0.3380\n",
      "Epoch 431/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8989 - loss: 0.3372\n",
      "Epoch 432/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8979 - loss: 0.3365\n",
      "Epoch 433/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8989 - loss: 0.3358\n",
      "Epoch 434/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9008 - loss: 0.3350\n",
      "Epoch 435/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8989 - loss: 0.3343\n",
      "Epoch 436/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9008 - loss: 0.3336\n",
      "Epoch 437/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9008 - loss: 0.3329\n",
      "Epoch 438/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9008 - loss: 0.3322\n",
      "Epoch 439/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9008 - loss: 0.3314\n",
      "Epoch 440/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9027 - loss: 0.3307\n",
      "Epoch 441/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9027 - loss: 0.3300\n",
      "Epoch 442/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9027 - loss: 0.3293\n",
      "Epoch 443/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9008 - loss: 0.3286\n",
      "Epoch 444/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9027 - loss: 0.3278\n",
      "Epoch 445/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9027 - loss: 0.3271\n",
      "Epoch 446/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9027 - loss: 0.3264\n",
      "Epoch 447/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9027 - loss: 0.3257\n",
      "Epoch 448/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9017 - loss: 0.3250\n",
      "Epoch 449/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9036 - loss: 0.3243\n",
      "Epoch 450/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9046 - loss: 0.3236\n",
      "Epoch 451/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9046 - loss: 0.3229\n",
      "Epoch 452/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9046 - loss: 0.3222\n",
      "Epoch 453/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9046 - loss: 0.3215\n",
      "Epoch 454/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9046 - loss: 0.3208\n",
      "Epoch 455/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9046 - loss: 0.3201\n",
      "Epoch 456/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9046 - loss: 0.3195\n",
      "Epoch 457/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9046 - loss: 0.3188\n",
      "Epoch 458/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9055 - loss: 0.3181\n",
      "Epoch 459/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9084 - loss: 0.3174\n",
      "Epoch 460/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9065 - loss: 0.3167\n",
      "Epoch 461/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9074 - loss: 0.3160\n",
      "Epoch 462/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9084 - loss: 0.3154\n",
      "Epoch 463/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9094 - loss: 0.3147\n",
      "Epoch 464/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9113 - loss: 0.3140\n",
      "Epoch 465/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9103 - loss: 0.3134\n",
      "Epoch 466/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9113 - loss: 0.3127\n",
      "Epoch 467/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9141 - loss: 0.3121\n",
      "Epoch 468/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9113 - loss: 0.3114\n",
      "Epoch 469/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9151 - loss: 0.3107\n",
      "Epoch 470/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9151 - loss: 0.3101\n",
      "Epoch 471/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9151 - loss: 0.3095\n",
      "Epoch 472/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9151 - loss: 0.3088\n",
      "Epoch 473/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9151 - loss: 0.3082\n",
      "Epoch 474/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9160 - loss: 0.3075\n",
      "Epoch 475/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9160 - loss: 0.3069\n",
      "Epoch 476/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9160 - loss: 0.3063\n",
      "Epoch 477/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9151 - loss: 0.3056\n",
      "Epoch 478/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9141 - loss: 0.3050\n",
      "Epoch 479/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9141 - loss: 0.3043\n",
      "Epoch 480/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9170 - loss: 0.3037\n",
      "Epoch 481/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9179 - loss: 0.3031\n",
      "Epoch 482/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9179 - loss: 0.3024\n",
      "Epoch 483/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9179 - loss: 0.3018\n",
      "Epoch 484/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9160 - loss: 0.3011\n",
      "Epoch 485/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9179 - loss: 0.3005\n",
      "Epoch 486/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9160 - loss: 0.2998\n",
      "Epoch 487/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9160 - loss: 0.2992\n",
      "Epoch 488/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9170 - loss: 0.2986\n",
      "Epoch 489/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9160 - loss: 0.2979\n",
      "Epoch 490/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9160 - loss: 0.2973\n",
      "Epoch 491/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9189 - loss: 0.2967\n",
      "Epoch 492/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9170 - loss: 0.2960\n",
      "Epoch 493/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9170 - loss: 0.2954\n",
      "Epoch 494/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9170 - loss: 0.2947\n",
      "Epoch 495/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9179 - loss: 0.2941\n",
      "Epoch 496/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9170 - loss: 0.2935\n",
      "Epoch 497/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9170 - loss: 0.2928\n",
      "Epoch 498/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9170 - loss: 0.2922\n",
      "Epoch 499/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9170 - loss: 0.2916\n",
      "Epoch 500/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9179 - loss: 0.2910\n",
      "Epoch 501/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9170 - loss: 0.2903\n",
      "Epoch 502/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9179 - loss: 0.2897\n",
      "Epoch 503/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9179 - loss: 0.2891\n",
      "Epoch 504/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9189 - loss: 0.2885\n",
      "Epoch 505/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9189 - loss: 0.2879\n",
      "Epoch 506/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9198 - loss: 0.2873\n",
      "Epoch 507/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9198 - loss: 0.2867\n",
      "Epoch 508/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9189 - loss: 0.2860\n",
      "Epoch 509/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9198 - loss: 0.2855\n",
      "Epoch 510/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9208 - loss: 0.2848\n",
      "Epoch 511/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9208 - loss: 0.2842\n",
      "Epoch 512/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9218 - loss: 0.2836\n",
      "Epoch 513/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9227 - loss: 0.2831\n",
      "Epoch 514/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9218 - loss: 0.2824\n",
      "Epoch 515/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9218 - loss: 0.2818\n",
      "Epoch 516/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9218 - loss: 0.2812\n",
      "Epoch 517/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9246 - loss: 0.2806\n",
      "Epoch 518/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9218 - loss: 0.2801\n",
      "Epoch 519/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9237 - loss: 0.2795\n",
      "Epoch 520/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9256 - loss: 0.2789\n",
      "Epoch 521/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9265 - loss: 0.2783\n",
      "Epoch 522/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9284 - loss: 0.2777\n",
      "Epoch 523/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9275 - loss: 0.2772\n",
      "Epoch 524/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9265 - loss: 0.2766\n",
      "Epoch 525/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9265 - loss: 0.2761\n",
      "Epoch 526/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9284 - loss: 0.2754\n",
      "Epoch 527/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9284 - loss: 0.2749\n",
      "Epoch 528/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9275 - loss: 0.2743\n",
      "Epoch 529/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9275 - loss: 0.2738\n",
      "Epoch 530/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9265 - loss: 0.2732\n",
      "Epoch 531/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9275 - loss: 0.2727\n",
      "Epoch 532/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9275 - loss: 0.2721\n",
      "Epoch 533/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9275 - loss: 0.2715\n",
      "Epoch 534/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9303 - loss: 0.2710\n",
      "Epoch 535/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9303 - loss: 0.2705\n",
      "Epoch 536/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9313 - loss: 0.2699\n",
      "Epoch 537/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9313 - loss: 0.2694\n",
      "Epoch 538/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9313 - loss: 0.2688\n",
      "Epoch 539/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9313 - loss: 0.2683\n",
      "Epoch 540/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9313 - loss: 0.2677\n",
      "Epoch 541/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9342 - loss: 0.2671\n",
      "Epoch 542/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9342 - loss: 0.2666\n",
      "Epoch 543/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9323 - loss: 0.2660\n",
      "Epoch 544/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9313 - loss: 0.2654\n",
      "Epoch 545/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9342 - loss: 0.2648\n",
      "Epoch 546/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9342 - loss: 0.2643\n",
      "Epoch 547/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9342 - loss: 0.2637\n",
      "Epoch 548/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9342 - loss: 0.2632\n",
      "Epoch 549/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9313 - loss: 0.2627\n",
      "Epoch 550/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9323 - loss: 0.2621\n",
      "Epoch 551/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9332 - loss: 0.2616\n",
      "Epoch 552/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9361 - loss: 0.2611\n",
      "Epoch 553/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9361 - loss: 0.2605\n",
      "Epoch 554/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9342 - loss: 0.2600\n",
      "Epoch 555/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9351 - loss: 0.2594\n",
      "Epoch 556/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9361 - loss: 0.2589\n",
      "Epoch 557/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9361 - loss: 0.2584\n",
      "Epoch 558/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9342 - loss: 0.2578\n",
      "Epoch 559/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9361 - loss: 0.2573\n",
      "Epoch 560/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9361 - loss: 0.2568\n",
      "Epoch 561/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9361 - loss: 0.2563\n",
      "Epoch 562/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9361 - loss: 0.2558\n",
      "Epoch 563/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9361 - loss: 0.2552\n",
      "Epoch 564/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9361 - loss: 0.2547\n",
      "Epoch 565/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9370 - loss: 0.2542\n",
      "Epoch 566/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9361 - loss: 0.2537\n",
      "Epoch 567/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9342 - loss: 0.2532\n",
      "Epoch 568/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9370 - loss: 0.2527\n",
      "Epoch 569/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9380 - loss: 0.2522\n",
      "Epoch 570/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9380 - loss: 0.2517\n",
      "Epoch 571/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9370 - loss: 0.2512\n",
      "Epoch 572/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9380 - loss: 0.2507\n",
      "Epoch 573/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9380 - loss: 0.2502\n",
      "Epoch 574/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9370 - loss: 0.2497\n",
      "Epoch 575/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9389 - loss: 0.2492\n",
      "Epoch 576/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9380 - loss: 0.2487\n",
      "Epoch 577/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9389 - loss: 0.2482\n",
      "Epoch 578/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9389 - loss: 0.2477\n",
      "Epoch 579/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9380 - loss: 0.2473\n",
      "Epoch 580/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9389 - loss: 0.2468\n",
      "Epoch 581/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9389 - loss: 0.2463\n",
      "Epoch 582/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9389 - loss: 0.2458\n",
      "Epoch 583/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9389 - loss: 0.2453\n",
      "Epoch 584/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9389 - loss: 0.2448\n",
      "Epoch 585/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9389 - loss: 0.2444\n",
      "Epoch 586/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9389 - loss: 0.2439\n",
      "Epoch 587/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9389 - loss: 0.2434\n",
      "Epoch 588/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9389 - loss: 0.2430\n",
      "Epoch 589/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9389 - loss: 0.2425\n",
      "Epoch 590/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9389 - loss: 0.2420\n",
      "Epoch 591/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9389 - loss: 0.2415\n",
      "Epoch 592/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9389 - loss: 0.2411\n",
      "Epoch 593/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9389 - loss: 0.2406\n",
      "Epoch 594/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9389 - loss: 0.2402\n",
      "Epoch 595/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9389 - loss: 0.2397\n",
      "Epoch 596/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9389 - loss: 0.2392\n",
      "Epoch 597/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9389 - loss: 0.2388\n",
      "Epoch 598/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9389 - loss: 0.2383\n",
      "Epoch 599/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9380 - loss: 0.2378\n",
      "Epoch 600/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9418 - loss: 0.2374\n",
      "Epoch 601/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9408 - loss: 0.2369\n",
      "Epoch 602/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9408 - loss: 0.2364\n",
      "Epoch 603/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9408 - loss: 0.2359\n",
      "Epoch 604/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9408 - loss: 0.2354\n",
      "Epoch 605/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9399 - loss: 0.2350\n",
      "Epoch 606/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9399 - loss: 0.2345\n",
      "Epoch 607/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9399 - loss: 0.2341\n",
      "Epoch 608/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9399 - loss: 0.2336\n",
      "Epoch 609/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9408 - loss: 0.2331\n",
      "Epoch 610/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9399 - loss: 0.2327\n",
      "Epoch 611/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9399 - loss: 0.2322\n",
      "Epoch 612/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9418 - loss: 0.2318\n",
      "Epoch 613/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9408 - loss: 0.2313\n",
      "Epoch 614/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9427 - loss: 0.2309\n",
      "Epoch 615/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9427 - loss: 0.2304\n",
      "Epoch 616/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9437 - loss: 0.2300\n",
      "Epoch 617/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9427 - loss: 0.2295\n",
      "Epoch 618/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9418 - loss: 0.2291\n",
      "Epoch 619/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9437 - loss: 0.2286\n",
      "Epoch 620/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9437 - loss: 0.2282\n",
      "Epoch 621/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9427 - loss: 0.2277\n",
      "Epoch 622/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9427 - loss: 0.2273\n",
      "Epoch 623/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9427 - loss: 0.2269\n",
      "Epoch 624/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9437 - loss: 0.2264\n",
      "Epoch 625/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9427 - loss: 0.2260\n",
      "Epoch 626/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9427 - loss: 0.2255\n",
      "Epoch 627/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9437 - loss: 0.2251\n",
      "Epoch 628/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9437 - loss: 0.2246\n",
      "Epoch 629/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9427 - loss: 0.2243\n",
      "Epoch 630/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9437 - loss: 0.2238\n",
      "Epoch 631/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9437 - loss: 0.2234\n",
      "Epoch 632/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9437 - loss: 0.2230\n",
      "Epoch 633/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9447 - loss: 0.2225\n",
      "Epoch 634/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9437 - loss: 0.2221\n",
      "Epoch 635/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9437 - loss: 0.2217\n",
      "Epoch 636/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9437 - loss: 0.2213\n",
      "Epoch 637/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9447 - loss: 0.2208\n",
      "Epoch 638/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9437 - loss: 0.2204\n",
      "Epoch 639/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9437 - loss: 0.2200\n",
      "Epoch 640/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9437 - loss: 0.2196\n",
      "Epoch 641/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9456 - loss: 0.2192\n",
      "Epoch 642/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9437 - loss: 0.2187\n",
      "Epoch 643/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9447 - loss: 0.2183\n",
      "Epoch 644/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9447 - loss: 0.2179\n",
      "Epoch 645/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9456 - loss: 0.2175\n",
      "Epoch 646/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9456 - loss: 0.2171\n",
      "Epoch 647/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9437 - loss: 0.2167\n",
      "Epoch 648/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9466 - loss: 0.2163\n",
      "Epoch 649/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9466 - loss: 0.2159\n",
      "Epoch 650/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9466 - loss: 0.2154\n",
      "Epoch 651/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9475 - loss: 0.2151\n",
      "Epoch 652/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9485 - loss: 0.2147\n",
      "Epoch 653/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9475 - loss: 0.2143\n",
      "Epoch 654/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9475 - loss: 0.2138\n",
      "Epoch 655/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9485 - loss: 0.2135\n",
      "Epoch 656/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9475 - loss: 0.2131\n",
      "Epoch 657/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9485 - loss: 0.2127\n",
      "Epoch 658/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9485 - loss: 0.2123\n",
      "Epoch 659/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9485 - loss: 0.2119\n",
      "Epoch 660/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9475 - loss: 0.2115\n",
      "Epoch 661/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9485 - loss: 0.2111\n",
      "Epoch 662/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9504 - loss: 0.2107\n",
      "Epoch 663/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9475 - loss: 0.2103\n",
      "Epoch 664/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9513 - loss: 0.2099\n",
      "Epoch 665/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9513 - loss: 0.2095\n",
      "Epoch 666/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9485 - loss: 0.2091\n",
      "Epoch 667/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9513 - loss: 0.2087\n",
      "Epoch 668/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9485 - loss: 0.2083\n",
      "Epoch 669/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9513 - loss: 0.2079\n",
      "Epoch 670/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9513 - loss: 0.2075\n",
      "Epoch 671/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9513 - loss: 0.2071\n",
      "Epoch 672/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9513 - loss: 0.2067\n",
      "Epoch 673/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9513 - loss: 0.2064\n",
      "Epoch 674/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9523 - loss: 0.2060\n",
      "Epoch 675/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9504 - loss: 0.2056\n",
      "Epoch 676/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9523 - loss: 0.2052\n",
      "Epoch 677/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9513 - loss: 0.2048\n",
      "Epoch 678/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9504 - loss: 0.2044\n",
      "Epoch 679/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9513 - loss: 0.2040\n",
      "Epoch 680/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9523 - loss: 0.2037\n",
      "Epoch 681/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9513 - loss: 0.2033\n",
      "Epoch 682/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9523 - loss: 0.2029\n",
      "Epoch 683/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9523 - loss: 0.2026\n",
      "Epoch 684/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9523 - loss: 0.2021\n",
      "Epoch 685/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9532 - loss: 0.2018\n",
      "Epoch 686/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9523 - loss: 0.2015\n",
      "Epoch 687/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9513 - loss: 0.2011\n",
      "Epoch 688/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9523 - loss: 0.2007\n",
      "Epoch 689/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9523 - loss: 0.2003\n",
      "Epoch 690/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9542 - loss: 0.2000\n",
      "Epoch 691/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9532 - loss: 0.1996\n",
      "Epoch 692/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9552 - loss: 0.1993\n",
      "Epoch 693/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9542 - loss: 0.1989\n",
      "Epoch 694/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9542 - loss: 0.1985\n",
      "Epoch 695/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9552 - loss: 0.1982\n",
      "Epoch 696/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9542 - loss: 0.1978\n",
      "Epoch 697/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9552 - loss: 0.1974\n",
      "Epoch 698/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9561 - loss: 0.1971\n",
      "Epoch 699/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9552 - loss: 0.1967\n",
      "Epoch 700/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9561 - loss: 0.1964\n",
      "Epoch 701/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9552 - loss: 0.1960\n",
      "Epoch 702/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9561 - loss: 0.1957\n",
      "Epoch 703/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9552 - loss: 0.1953\n",
      "Epoch 704/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9552 - loss: 0.1950\n",
      "Epoch 705/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9552 - loss: 0.1946\n",
      "Epoch 706/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9561 - loss: 0.1943\n",
      "Epoch 707/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9552 - loss: 0.1939\n",
      "Epoch 708/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9561 - loss: 0.1936\n",
      "Epoch 709/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9561 - loss: 0.1932\n",
      "Epoch 710/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9542 - loss: 0.1929\n",
      "Epoch 711/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9561 - loss: 0.1926\n",
      "Epoch 712/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9552 - loss: 0.1923\n",
      "Epoch 713/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9561 - loss: 0.1920\n",
      "Epoch 714/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9552 - loss: 0.1916\n",
      "Epoch 715/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9561 - loss: 0.1912\n",
      "Epoch 716/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9561 - loss: 0.1909\n",
      "Epoch 717/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9552 - loss: 0.1906\n",
      "Epoch 718/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9552 - loss: 0.1903\n",
      "Epoch 719/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9561 - loss: 0.1900\n",
      "Epoch 720/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9552 - loss: 0.1896\n",
      "Epoch 721/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9552 - loss: 0.1893\n",
      "Epoch 722/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9571 - loss: 0.1890\n",
      "Epoch 723/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9561 - loss: 0.1886\n",
      "Epoch 724/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9542 - loss: 0.1883\n",
      "Epoch 725/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9552 - loss: 0.1880\n",
      "Epoch 726/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9571 - loss: 0.1877\n",
      "Epoch 727/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9552 - loss: 0.1874\n",
      "Epoch 728/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9552 - loss: 0.1870\n",
      "Epoch 729/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9580 - loss: 0.1867\n",
      "Epoch 730/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9561 - loss: 0.1864\n",
      "Epoch 731/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9542 - loss: 0.1861\n",
      "Epoch 732/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9580 - loss: 0.1858\n",
      "Epoch 733/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9580 - loss: 0.1855\n",
      "Epoch 734/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9561 - loss: 0.1852\n",
      "Epoch 735/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9571 - loss: 0.1848\n",
      "Epoch 736/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9571 - loss: 0.1845\n",
      "Epoch 737/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9571 - loss: 0.1842\n",
      "Epoch 738/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9571 - loss: 0.1839\n",
      "Epoch 739/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9571 - loss: 0.1836\n",
      "Epoch 740/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9580 - loss: 0.1832\n",
      "Epoch 741/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9580 - loss: 0.1829\n",
      "Epoch 742/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9590 - loss: 0.1826\n",
      "Epoch 743/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9580 - loss: 0.1823\n",
      "Epoch 744/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9580 - loss: 0.1820\n",
      "Epoch 745/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9580 - loss: 0.1817\n",
      "Epoch 746/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9580 - loss: 0.1814\n",
      "Epoch 747/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9580 - loss: 0.1811\n",
      "Epoch 748/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9590 - loss: 0.1808\n",
      "Epoch 749/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9571 - loss: 0.1805\n",
      "Epoch 750/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9590 - loss: 0.1802\n",
      "Epoch 751/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9599 - loss: 0.1799\n",
      "Epoch 752/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9599 - loss: 0.1796\n",
      "Epoch 753/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9590 - loss: 0.1792\n",
      "Epoch 754/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9599 - loss: 0.1789\n",
      "Epoch 755/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9590 - loss: 0.1786\n",
      "Epoch 756/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9590 - loss: 0.1783\n",
      "Epoch 757/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9590 - loss: 0.1780\n",
      "Epoch 758/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9599 - loss: 0.1777\n",
      "Epoch 759/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9599 - loss: 0.1774\n",
      "Epoch 760/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9590 - loss: 0.1771\n",
      "Epoch 761/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9599 - loss: 0.1768\n",
      "Epoch 762/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9599 - loss: 0.1765\n",
      "Epoch 763/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9590 - loss: 0.1762\n",
      "Epoch 764/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9599 - loss: 0.1759\n",
      "Epoch 765/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9599 - loss: 0.1757\n",
      "Epoch 766/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9599 - loss: 0.1754\n",
      "Epoch 767/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9590 - loss: 0.1751\n",
      "Epoch 768/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9599 - loss: 0.1748\n",
      "Epoch 769/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9599 - loss: 0.1744\n",
      "Epoch 770/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9599 - loss: 0.1741\n",
      "Epoch 771/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9599 - loss: 0.1738\n",
      "Epoch 772/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9599 - loss: 0.1735\n",
      "Epoch 773/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9599 - loss: 0.1732\n",
      "Epoch 774/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9609 - loss: 0.1730\n",
      "Epoch 775/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9609 - loss: 0.1727\n",
      "Epoch 776/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9609 - loss: 0.1723\n",
      "Epoch 777/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9599 - loss: 0.1721\n",
      "Epoch 778/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9599 - loss: 0.1718\n",
      "Epoch 779/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9609 - loss: 0.1715\n",
      "Epoch 780/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9609 - loss: 0.1712\n",
      "Epoch 781/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9609 - loss: 0.1710\n",
      "Epoch 782/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9609 - loss: 0.1707\n",
      "Epoch 783/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9609 - loss: 0.1704\n",
      "Epoch 784/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9609 - loss: 0.1701\n",
      "Epoch 785/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9609 - loss: 0.1698\n",
      "Epoch 786/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9609 - loss: 0.1695\n",
      "Epoch 787/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9609 - loss: 0.1692\n",
      "Epoch 788/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9609 - loss: 0.1690\n",
      "Epoch 789/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9609 - loss: 0.1686\n",
      "Epoch 790/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9609 - loss: 0.1683\n",
      "Epoch 791/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9609 - loss: 0.1681\n",
      "Epoch 792/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9609 - loss: 0.1678\n",
      "Epoch 793/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9609 - loss: 0.1676\n",
      "Epoch 794/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9609 - loss: 0.1674\n",
      "Epoch 795/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9609 - loss: 0.1671\n",
      "Epoch 796/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9618 - loss: 0.1668\n",
      "Epoch 797/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9609 - loss: 0.1665\n",
      "Epoch 798/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9609 - loss: 0.1662\n",
      "Epoch 799/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.9609 - loss: 0.1659\n",
      "Epoch 800/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9609 - loss: 0.1656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x193c76fe940>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Define the neural network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(17, 2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # Adjust the number of output units to match your number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(trained_data, labels, epochs=800, batch_size=1048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ed406a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:27:28.540162Z",
     "iopub.status.busy": "2024-07-12T13:27:28.539780Z",
     "iopub.status.idle": "2024-07-12T13:27:28.672249Z",
     "shell.execute_reply": "2024-07-12T13:27:28.671255Z"
    },
    "papermill": {
     "duration": 0.543054,
     "end_time": "2024-07-12T13:27:28.674440",
     "exception": false,
     "start_time": "2024-07-12T13:27:28.131386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "for i in predicted_classes:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807f63c",
   "metadata": {
    "papermill": {
     "duration": 0.391683,
     "end_time": "2024-07-12T13:27:29.499729",
     "exception": false,
     "start_time": "2024-07-12T13:27:29.108046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 920599,
     "sourceId": 1559111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5378310,
     "sourceId": 8938815,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 64306,
     "sourceId": 76515,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 89.878931,
   "end_time": "2024-07-12T13:27:31.412275",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-12T13:26:01.533344",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
